<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
    
<link rel="stylesheet" href="/localshare/css/share.css">

  
  
<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        
          <a class="main-nav-link" href="/."><i class="fa fa-home"></i> Home</a>
        
          <a class="main-nav-link" href="/archives/"><i class="fa fa-archive"></i> Archive</a>
        
          <a class="main-nav-link" href="/about/"><i class="fa fa-user"></i> About</a>
        
          <a class="main-nav-link" href="/atom.xml"><i class="fa fa-rss"></i> RSS</a>
        
      </nav>
    </div>
    <div id="search-form">
      <div id="result-mask" class="hide"></div>
      <label><input id="search-key" type="text" autocomplete="off" placeholder="search"></label>
      <div id="result-wrap" class="hide">
        <div id="search-result"></div>
      </div>
      <div class="hide">
        <template id="search-tpl">
          <div class="item">
            <a href="/{path}" title="{title}">
              <div class="title">{title}</div>
              <div class="time">{date}</div>
              <div class="tags">{tags}</div>
            </a>
          </div>
        </template>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  <article id="post-Python-100-Days-master/Day66-70/67.NumPy的应用" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-inner">
    
    
    <div class="article-meta">
      
      <span class="article-date">
  <i class="fa fa-date"></i>
  <time class="dt-published" datetime="2021-10-17T13:24:55.772Z" itemprop="datePublished">2021年10月17日</time>
</span>
      
      
      
<a href="/2021/10/17/Python-100-Days-master/Day66-70/67.NumPy%E7%9A%84%E5%BA%94%E7%94%A8/#comments" class="article-comment-link">
  
  <i class="fa fa-commt"></i>
  Guestbook
</a>


    </div>
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="NumPy的应用"><a href="#NumPy的应用" class="headerlink" title="NumPy的应用"></a>NumPy的应用</h2><p>Numpy是一个开源的Python科学计算库，<strong>用于快速处理任意维度的数组</strong>。Numpy<strong>支持常见的数组和矩阵操作</strong>，对于同样的数值计算任务，使用NumPy不仅代码要简洁的多，而且NumPy的性能远远优于原生Python，基本是一个到两个数量级的差距，而且数据量越大，NumPy的优势就越明显。</p>
<p>Numpy最为核心的数据类型是<code>ndarray</code>，使用<code>ndarray</code>可以处理一维、二维和多维数组，该对象相当于是一个快速而灵活的大数据容器。NumPy底层代码使用C语言编写，解决了GIL的限制，<code>ndarray</code>在存储数据的时候，数据与数据的地址都是连续的，这样就给使得批量操作速度很快，远远优于Python中的<code>list</code>；另一方面<code>ndarray</code>对象提供了更多的方法来处理数据，尤其是和统计相关的方法，这些方法也是Python原生的<code>list</code>没有的。</p>
<h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><ol>
<li><p>启动Notebook</p>
<pre><code class="Bash">jupyter notebook
</code></pre>
<blockquote>
<p><strong>提示</strong>：在启动Notebook之前，建议先安装好数据分析相关依赖项，包括之前提到的三大神器以及相关依赖项，包括：<code>numpy</code>、<code>pandas</code>、<code>matplotlib</code>、<code>openpyxl</code>、<code>xlrd</code>、<code>xlwt</code>等。如果使用Anaconda，则无需单独安装。</p>
</blockquote>
</li>
<li><p>导入</p>
<pre><code class="Python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
</code></pre>
<blockquote>
<p><strong>说明</strong>：如果已经启动了Notebook但尚未安装相关依赖库，例如NumPy，可以在Notebook的单元格中输入<code>!pip install numpy</code>并运行该单元格来安装NumPy，其他库如法炮制。安装成功后选择“Kernel”（服务）菜单的“Restart”（重启）选项来重启Notebook内核（前面有讲到重启的快捷键）来使新安装的库生效。上面我们不仅导入了NumPy，还将pandas和matplotlib库一并导入了。</p>
</blockquote>
</li>
</ol>
<h3 id="创建数组对象"><a href="#创建数组对象" class="headerlink" title="创建数组对象"></a>创建数组对象</h3><p>创建<code>ndarray</code>对象有很多种方法，下面就如何创建一维数组、二维数组和多维数组进行说明。</p>
<h4 id="一维数组"><a href="#一维数组" class="headerlink" title="一维数组"></a>一维数组</h4><ul>
<li><p>方法一：使用<code>array</code>函数，通过<code>list</code>创建数组对象</p>
<p>  代码：</p>
<pre><code class="Python">array1 = np.array([1, 2, 3, 4, 5])
array1
</code></pre>
<p>  输出：</p>
<pre><code>array([1, 2, 3, 4, 5])
</code></pre>
</li>
<li><p>方法二：使用<code>arange</code>函数，指定取值范围创建数组对象</p>
<p>  代码：</p>
<pre><code class="Python">array2 = np.arange(0, 20, 2)
array2
</code></pre>
<p>  输出：</p>
<pre><code>array([ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18])
</code></pre>
</li>
<li><p>方法三：使用<code>linspace</code>函数，用指定范围均匀间隔的数字创建数组对象</p>
<p>  代码：</p>
<pre><code class="Python">array3 = np.linspace(-5, 5, 101)
array3
</code></pre>
<p>  输出：</p>
<pre><code>array([-5. , -4.9, -4.8, -4.7, -4.6, -4.5, -4.4, -4.3, -4.2, -4.1, -4. ,
       -3.9, -3.8, -3.7, -3.6, -3.5, -3.4, -3.3, -3.2, -3.1, -3. , -2.9,
       -2.8, -2.7, -2.6, -2.5, -2.4, -2.3, -2.2, -2.1, -2. , -1.9, -1.8,
       -1.7, -1.6, -1.5, -1.4, -1.3, -1.2, -1.1, -1. , -0.9, -0.8, -0.7,
       -0.6, -0.5, -0.4, -0.3, -0.2, -0.1,  0. ,  0.1,  0.2,  0.3,  0.4,
        0.5,  0.6,  0.7,  0.8,  0.9,  1. ,  1.1,  1.2,  1.3,  1.4,  1.5,
        1.6,  1.7,  1.8,  1.9,  2. ,  2.1,  2.2,  2.3,  2.4,  2.5,  2.6,
        2.7,  2.8,  2.9,  3. ,  3.1,  3.2,  3.3,  3.4,  3.5,  3.6,  3.7,
        3.8,  3.9,  4. ,  4.1,  4.2,  4.3,  4.4,  4.5,  4.6,  4.7,  4.8,
        4.9,  5. ])
</code></pre>
</li>
<li><p>方法四：使用<code>numpy.random</code>模块的函数生成随机数创建数组对象</p>
<p>  产生10个$[0, 1)$范围的随机小数，代码：</p>
<pre><code class="Python">array4 = np.random.rand(10)
array4
</code></pre>
<p>  输出：</p>
<pre><code>array([0.45556132, 0.67871326, 0.4552213 , 0.96671509, 0.44086463,
       0.72650875, 0.79877188, 0.12153022, 0.24762739, 0.6669852 ])
</code></pre>
<p>  产生10个$[1, 100)$范围的随机整数，代码：</p>
<pre><code class="Python">array5 = np.random.randint(1, 100, 10)
array5
</code></pre>
<p>  输出：</p>
<pre><code>array([29, 97, 87, 47, 39, 19, 71, 32, 79, 34])
</code></pre>
<p>  产生20个$\mu=50$，$\sigma=10$的正态分布随机数，代码：</p>
<pre><code class="Python">array6 = np.random.normal(50, 10, 20)
array6
</code></pre>
<p>  输出：</p>
<pre><code>array([55.04155586, 46.43510797, 20.28371158, 62.67884053, 61.23185964,
       38.22682148, 53.17126151, 43.54741592, 36.11268017, 40.94086676,
       63.27911699, 46.92688903, 37.1593374 , 67.06525656, 67.47269463,
       23.37925889, 31.45312239, 48.34532466, 55.09180924, 47.95702787])
</code></pre>
</li>
</ul>
<blockquote>
<p><strong>说明</strong>：创建一维数组还有很多其他的方式，比如通过读取字符串、读取文件、解析正则表达式等方式，这里我们暂不讨论这些方式，有兴趣的读者可以自行研究。</p>
</blockquote>
<h4 id="二维数组"><a href="#二维数组" class="headerlink" title="二维数组"></a>二维数组</h4><ul>
<li><p>方法一：使用<code>array</code>函数，通过嵌套的<code>list</code>创建数组对象</p>
<p>  代码：</p>
<pre><code class="Python">array7 = np.array([[1, 2, 3], [4, 5, 6]])
array7
</code></pre>
<p>  输出：</p>
<pre><code>array([[1, 2, 3],
       [4, 5, 6]])
</code></pre>
</li>
<li><p>方法二：使用<code>zeros</code>、<code>ones</code>、<code>full</code>函数指定数组的形状创建数组对象</p>
<p>  使用<code>zeros</code>函数，代码：</p>
<pre><code class="Python">array8 = np.zeros((3, 4))
array8
</code></pre>
<p>  输出：</p>
<pre><code>array([[0., 0., 0., 0.],
       [0., 0., 0., 0.],
       [0., 0., 0., 0.]])
</code></pre>
<p>  使用<code>ones</code>函数，代码：</p>
<pre><code class="Python">array9 = np.ones((3, 4))
array9
</code></pre>
<p>  输出：</p>
<pre><code>array([[1., 1., 1., 1.],
       [1., 1., 1., 1.],
       [1., 1., 1., 1.]])
</code></pre>
<p>  使用<code>full</code>函数，代码：</p>
<pre><code class="Python">array10 = np.full((3, 4), 10)
array10
</code></pre>
<p>  输出：</p>
<pre><code>array([[10, 10, 10, 10],
       [10, 10, 10, 10],
       [10, 10, 10, 10]])
</code></pre>
</li>
<li><p>方法三：使用eye函数创建单位矩阵</p>
<p>  代码：</p>
<pre><code class="Python">array11 = np.eye(4)
array11
</code></pre>
<p>  输出：</p>
<pre><code>array([[1., 0., 0., 0.],
       [0., 1., 0., 0.],
       [0., 0., 1., 0.],
       [0., 0., 0., 1.]])
</code></pre>
</li>
<li><p>方法四：通过<code>reshape</code>将一维数组变成二维数组</p>
<p>  代码：</p>
<pre><code class="Python">array12 = np.array([1, 2, 3, 4, 5, 6]).reshape(2, 3)
array12
</code></pre>
<p>  输出：</p>
<pre><code>array([[1, 2, 3],
       [4, 5, 6]])
</code></pre>
<blockquote>
<p><strong>提示</strong>：<code>reshape</code>是<code>ndarray</code>对象的一个方法，使用<code>reshape</code>方法时需要确保调形后的数组元素个数与调形前数组元素个数保持一致，否则将会产生异常。</p>
</blockquote>
</li>
<li><p>方法五：通过<code>numpy.random</code>模块的函数生成随机数创建数组对象</p>
<p>  产生$[0, 1)$范围的随机小数构成的3行4列的二维数组，代码：</p>
<pre><code class="Python">array13 = np.random.rand(3, 4)
array13
</code></pre>
<p>  输出：</p>
<pre><code>array([[0.54017809, 0.46797771, 0.78291445, 0.79501326],
       [0.93973783, 0.21434806, 0.03592874, 0.88838892],
       [0.84130479, 0.3566601 , 0.99935473, 0.26353598]])
</code></pre>
<p>  产生$[1, 100)$范围的随机整数构成的3行4列的二维数组，代码：</p>
<pre><code class="Python">array14 = np.random.randint(1, 100, (3, 4))
array14
</code></pre>
<p>  输出：</p>
<pre><code>array([[83, 30, 64, 53],
       [39, 92, 53, 43],
       [43, 48, 91, 72]])
</code></pre>
</li>
</ul>
<h4 id="多维数组"><a href="#多维数组" class="headerlink" title="多维数组"></a>多维数组</h4><ul>
<li><p>使用随机的方式创建多维数组</p>
<p>  代码：</p>
<pre><code class="Python">array15 = np.random.randint(1, 100, (3, 4, 5))
array15
</code></pre>
<p>  输出：</p>
<pre><code>array([[[94, 26, 49, 24, 43],
        [27, 27, 33, 98, 33],
        [13, 73,  6,  1, 77],
        [54, 32, 51, 86, 59]],

       [[62, 75, 62, 29, 87],
        [90, 26,  6, 79, 41],
        [31, 15, 32, 56, 64],
        [37, 84, 61, 71, 71]],

       [[45, 24, 78, 77, 41],
        [75, 37,  4, 74, 93],
        [ 1, 36, 36, 60, 43],
        [23, 84, 44, 89, 79]]])
</code></pre>
</li>
<li><p>将一维二维的数组调形为多维数组</p>
<p>  一维数组调形为多维数组，代码：</p>
<pre><code class="Python">array16 = np.arange(1, 25).reshape((2, 3, 4))
array16
</code></pre>
<p>  输出：</p>
<pre><code class="Python">array([[[ 1,  2,  3,  4],
        [ 5,  6,  7,  8],
        [ 9, 10, 11, 12]],

       [[13, 14, 15, 16],
        [17, 18, 19, 20],
        [21, 22, 23, 24]]])
</code></pre>
<p>  二维数组调形为多维数组，代码：</p>
<pre><code class="Python">array17 = np.random.randint(1, 100, (4, 6)).reshape((4, 3, 2))
array17
</code></pre>
<p>  输出：</p>
<pre><code>array([[[60, 59],
        [31, 80],
        [54, 91]],

       [[67,  4],
        [ 4, 59],
        [47, 49]],

       [[16,  4],
        [ 5, 71],
        [80, 53]],

       [[38, 49],
        [70,  5],
        [76, 80]]])
</code></pre>
</li>
<li><p>读取图片获得对应的三维数组</p>
<p>  代码：</p>
<pre><code class="Python">array18 = plt.imread(&#39;guido.jpg&#39;)
array18
</code></pre>
<p>  输出：</p>
<pre><code>array([[[ 36,  33,  28],
        [ 36,  33,  28],
        [ 36,  33,  28],
        ...,
        [ 32,  31,  29],
        [ 32,  31,  27],
        [ 31,  32,  26]],

       [[ 37,  34,  29],
        [ 38,  35,  30],
        [ 38,  35,  30],
        ...,
        [ 31,  30,  28],
        [ 31,  30,  26],
        [ 30,  31,  25]],

       [[ 38,  35,  30],
        [ 38,  35,  30],
        [ 38,  35,  30],
        ...,
        [ 30,  29,  27],
        [ 30,  29,  25],
        [ 29,  30,  25]],

       ...,

       [[239, 178, 123],
        [237, 176, 121],
        [235, 174, 119],
        ...,
        [ 78,  68,  56],
        [ 75,  67,  54],
        [ 73,  65,  52]],

       [[238, 177, 120],
        [236, 175, 118],
        [234, 173, 116],
        ...,
        [ 82,  70,  58],
        [ 78,  68,  56],
        [ 75,  66,  51]],

       [[238, 176, 119],
        [236, 175, 118],
        [234, 173, 116],
        ...,
        [ 84,  70,  61],
        [ 81,  69,  57],
        [ 79,  67,  53]]], dtype=uint8)
</code></pre>
<blockquote>
<p><strong>说明</strong>：上面的代码读取了当前路径下名为<code>guido.jpg</code> 的图片文件，计算机系统中的图片通常由若干行若干列的像素点构成，而每个像素点又是由红绿蓝三原色构成的，所以能够用三维数组来表示。读取图片用到了matplotlib库的<code>imread</code>函数。</p>
</blockquote>
</li>
</ul>
<h3 id="数组对象的属性"><a href="#数组对象的属性" class="headerlink" title="数组对象的属性"></a>数组对象的属性</h3><ol>
<li><p><code>size</code>属性：数组元素个数</p>
<p> 代码：</p>
<pre><code class="Python">array19 = np.arange(1, 100, 2)
array20 = np.random.rand(3, 4)
print(array19.size, array20.size)
</code></pre>
<p> 输出：</p>
<pre><code>50 12
</code></pre>
</li>
<li><p><code>shape</code>属性：数组的形状</p>
<p> 代码：</p>
<pre><code class="Python">print(array19.shape, array20.shape)
</code></pre>
<p> 输出：</p>
<pre><code>(50,) (3, 4)
</code></pre>
</li>
<li><p><code>dtype</code>属性：数组元素的数据类型</p>
<p> 代码：</p>
<pre><code class="Python">print(array19.dtype, array20.dtype)
</code></pre>
<p> 输出：</p>
<pre><code>int64 float64
</code></pre>
<p> <code>ndarray</code>对象元素的数据类型可以参考如下所示的表格。</p>
<p> <img src="res/ndarray-dtype.png"></p>
</li>
<li><p><code>ndim</code>属性：数组的维度</p>
<p> 代码：</p>
<pre><code class="Python">print(array19.ndim, array20.ndim)
</code></pre>
<p> 输出：</p>
<pre><code>1 2
</code></pre>
</li>
<li><p><code>itemsize</code>属性：数组单个元素占用内存空间的字节数</p>
<p> 代码：</p>
<pre><code class="Python">array21 = np.arange(1, 100, 2, dtype=np.int8)
print(array19.itemsize, array20.itemsize, array21.itemsize)
</code></pre>
<p> 输出：</p>
<pre><code>8 8 1
</code></pre>
<blockquote>
<p><strong>说明</strong>：在使用<code>arange</code>创建数组对象时，通过<code>dtype</code>参数指定元素的数据类型。可以看出，<code>np.int8</code>代表的是8位有符号整数，只占用1个字节的内存空间，取值范围是$[-128,127]$。</p>
</blockquote>
</li>
<li><p><code>nbytes</code>属性：数组所有元素占用内存空间的字节数</p>
<p> 代码：</p>
<pre><code class="Python">print(array19.nbytes, array20.nbytes, array21.nbytes)
</code></pre>
<p> 输出：</p>
<pre><code>400 96 50
</code></pre>
</li>
<li><p><code>flat</code>属性：数组（一维化之后）元素的迭代器</p>
<p> 代码：</p>
<pre><code class="Python">from typing import Iterable

print(isinstance(array20.flat, np.ndarray), isinstance(array20.flat, Iterable))
</code></pre>
<p> 输出：</p>
<pre><code>False True
</code></pre>
</li>
<li><p><code>base</code>属性：数组的基对象（如果数组共享了其他数组的内存空间）</p>
<p> 代码：</p>
<pre><code class="Python">array22 = array19[:]
print(array22.base is array19, array22.base is array21)
</code></pre>
<p> 输出：</p>
<pre><code>True False
</code></pre>
<blockquote>
<p><strong>说明</strong>：上面的代码用到了数组的切片操作，它类似于Python中<code>list</code>类型的切片，但在细节上又不完全相同，下面会专门讲解这个知识点。通过上面的代码可以发现，<code>ndarray</code>切片后得到的新的数组对象跟原来的数组对象共享了内存中的数据，因此<code>array22</code>的<code>base</code>属性就是<code>array19</code>对应的数组对象。</p>
</blockquote>
</li>
</ol>
<h3 id="数组的索引和切片"><a href="#数组的索引和切片" class="headerlink" title="数组的索引和切片"></a>数组的索引和切片</h3><p>和Python中的列表类似，NumPy的<code>ndarray</code>对象可以进行索引和切片操作，通过索引可以获取或修改数组中的元素，通过切片可以取出数组的一部分。</p>
<ol>
<li><p>索引运算（普通索引）</p>
<p> 一维数组，代码：</p>
<pre><code class="Python">array23 = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9])
print(array23[0], array23[array23.size - 1])
print(array23[-array23.size], array23[-1])
</code></pre>
<p> 输出：</p>
<pre><code>1 9
1 9
</code></pre>
<p> 二维数组，代码：</p>
<pre><code class="Python">array24 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
print(array24[2])
print(array24[0][0], array24[-1][-1])
print(array24[1][1], array24[1, 1])
</code></pre>
<p> 输出：</p>
<pre><code>[7 8 9]
1 9
5 5
[[ 1  2  3]
 [ 4 10  6]
 [ 7  8  9]]
</code></pre>
<p> 代码：</p>
<pre><code class="Python">array24[1][1] = 10
print(array24)
array24[1] = [10, 11, 12]
print(array24)
</code></pre>
<p> 输出：</p>
<pre><code>[[ 1  2  3]
 [ 4 10  6]
 [ 7  8  9]]
[[ 1  2  3]
 [10 11 12]
 [ 7  8  9]]
</code></pre>
</li>
<li><p>切片运算（切片索引）</p>
<p> 切片是形如<code>[开始索引:结束索引:步长]</code>的语法，通过指定<strong>开始索引</strong>（默认值无穷小）、<strong>结束索引</strong>（默认值无穷大）和<strong>步长</strong>（默认值1），从数组中取出指定部分的元素并构成新的数组。因为开始索引、结束索引和步长都有默认值，所以它们都可以省略，如果不指定步长，第二个冒号也可以省略。一维数组的切片运算跟Python中的<code>list</code>类型的切片非常类似，此处不再赘述，二维数组的切片可以参考下面的代码，相信非常容易理解。</p>
<p> 代码：</p>
<pre><code class="Python">print(array24[:2, 1:])
</code></pre>
<p> 输出：</p>
<pre><code>[[2 3]
 [5 6]]
</code></pre>
<p> 代码：</p>
<pre><code class="Python">print(array24[2])
print(array24[2, :])
</code></pre>
<p> 输出：</p>
<pre><code>[7 8 9]
[7 8 9]
</code></pre>
<p> 代码：</p>
<pre><code class="Python">print(array24[2:, :])
</code></pre>
<p> 输出：</p>
<pre><code>[[7 8 9]]
</code></pre>
<p> 代码：</p>
<pre><code class="Python">print(array24[:, :2])
</code></pre>
<p> 输出：</p>
<pre><code>[[1 2]
 [4 5]
 [7 8]]
</code></pre>
<p> 代码：</p>
<pre><code class="Python">print(array24[1, :2])
print(array24[1:2, :2])
</code></pre>
<p> 输出：</p>
<pre><code>[4 5]
[[4 5]]
</code></pre>
<p> 代码：</p>
<pre><code class="Python">print(array24[::2, ::2])
</code></pre>
<p> 输出：</p>
<pre><code>[[1 3]
 [7 9]]
</code></pre>
<p> 代码：</p>
<pre><code class="Python">print(array24[::-2, ::-2])
</code></pre>
<p> 输出：</p>
<pre><code>[[9 7]
 [3 1]]
</code></pre>
<p> 关于数组的索引和切片运算，大家可以通过下面的两张图来增强印象，这两张图来自<a target="_blank" rel="noopener" href="https://item.jd.com/12398725.html">《利用Python进行数据分析》</a>一书，它是pandas的作者Wes McKinney撰写的Python数据分析领域的经典教科书，有兴趣的读者可以购买和阅读原书。</p>
<p> <img src="res/ndarray-index.png"></p>
<p> <img src="res/ndarray-slice.png">  </p>
</li>
<li><p>花式索引（fancy index）</p>
<p> 花式索引（Fancy indexing）是指利用整数数组进行索引，这里所说的整数数组可以是NumPy的<code>ndarray</code>，也可以是Python中<code>list</code>、<code>tuple</code>等可迭代类型，可以使用正向或负向索引。</p>
<p> 一维数组的花式索引，代码：</p>
<pre><code class="Python">array25 = np.array([50, 30, 15, 20, 40])
array25[[0, 1, -1]]
</code></pre>
<p> 输出：</p>
<pre><code>array([50, 30, 40])
</code></pre>
<p> 二维数组的花式索引，代码：</p>
<pre><code class="Python">array26 = np.array([[30, 20, 10], [40, 60, 50], [10, 90, 80]])
# 取二维数组的第1行和第3行
array26[[0, 2]]
</code></pre>
<p> 输出：</p>
<pre><code>array([[30, 20, 10],
       [10, 90, 80]])
</code></pre>
<p> 代码：</p>
<pre><code class="Python"># 取二维数组第1行第2列，第3行第3列的两个元素
array26[[0, 2], [1, 2]]
</code></pre>
<p> 输出：</p>
<pre><code>array([20, 80])
</code></pre>
<p> 代码：</p>
<pre><code class="Python"># 取二维数组第1行第2列，第3行第2列的两个元素
array26[[0, 2], 1]
</code></pre>
<p> 输出：</p>
<pre><code>array([20, 90])
</code></pre>
</li>
<li><p>布尔索引</p>
<p> 布尔索引就是通过布尔类型的数组对数组元素进行索引，布尔类型的数组可以手动构造，也可以通过关系运算来产生布尔类型的数组。</p>
<p> 代码：</p>
<pre><code class="Python">array27 = np.arange(1, 10)
array27[[True, False, True, True, False, False, False, False, True]]
</code></pre>
<p> 输出：</p>
<pre><code>array([1, 3, 4, 9])
</code></pre>
<p> 代码：</p>
<pre><code class="Python">array27 &gt;= 5
</code></pre>
<p> 输出：</p>
<pre><code>array([False, False, False, False,  True,  True,  True,  True,  True])
</code></pre>
<p> 代码：</p>
<pre><code class="Python"># ~运算符可以实现逻辑变反，看看运行结果跟上面有什么不同
~(array27 &gt;= 5)
</code></pre>
<p> 输出：</p>
<pre><code>array([ True,  True,  True,  True, False, False, False, False, False])
</code></pre>
<p> 代码：</p>
<pre><code class="Python">array27[array27 &gt;= 5]
</code></pre>
<p> 输出：</p>
<pre><code>array([5, 6, 7, 8, 9])
</code></pre>
</li>
</ol>
<blockquote>
<p><strong>提示</strong>：切片操作虽然创建了新的数组对象，但是新数组和原数组共享了数组中的数据，简单的说，如果通过新数组对象或原数组对象修改数组中的数据，其实修改的是同一块数据。花式索引和布尔索引也会创建新的数组对象，而且新数组复制了原数组的元素，新数组和原数组并不是共享数据的关系，这一点通过前面讲的数组的<code>base</code>属性也可以了解到，大家一定要注意。</p>
</blockquote>
<h4 id="案例：通过数组切片处理图像"><a href="#案例：通过数组切片处理图像" class="headerlink" title="案例：通过数组切片处理图像"></a>案例：通过数组切片处理图像</h4><p>学习基础知识总是比较枯燥且没有成就感的，所以我们还是来个案例为大家演示下上面学习的数组索引和切片操作到底有什么用。前面我们说到过，可以用三维数组来表示图像，那么通过图像对应的三维数组进行操作，就可以实现对图像的处理，如下所示。</p>
<p>读入图片创建三维数组对象。</p>
<pre><code class="Python">guido_image = plt.imread(&#39;guido.jpg&#39;)
plt.imshow(guido_image)
</code></pre>
<p>对数组的0轴进行反向切片，实现图像的垂直翻转。</p>
<pre><code class="Python">plt.imshow(guido_image[::-1])
</code></pre>
<p><img src="res/image-flip-1.png"></p>
<p>对数组的1轴进行反向切片，实现图像的水平翻转。</p>
<pre><code class="Python">plt.imshow(guido_image[:,::-1])
</code></pre>
<p><img src="res/image-flip-2.png"></p>
<p>将Guido的头切出来。</p>
<pre><code class="Python">plt.imshow(guido_image[30:350, 90:300])
</code></pre>
<p><img src="res/image-flip-3.png"></p>
<h3 id="数组对象的方法"><a href="#数组对象的方法" class="headerlink" title="数组对象的方法"></a>数组对象的方法</h3><h4 id="统计方法"><a href="#统计方法" class="headerlink" title="统计方法"></a>统计方法</h4><p><code>ndarray</code>对象的统计方法主要包括：<code>sum</code>、<code>mean</code>、<code>std</code>、<code>var</code>、<code>min</code>、<code>max</code>、<code>argmin</code>、<code>argmax</code>、<code>cumsum</code>等，分别用于对数组中的元素求和、求平均、求标准差、求方差、找最大、找最小、求累积和等，请参考下面的代码。</p>
<pre><code class="Python">array28 = np.array([1, 2, 3, 4, 5, 5, 4, 3, 2, 1])
print(array28.sum())
print(array28.mean())
print(array28.max())
print(array28.min())
print(array28.std())
print(array28.var())
print(array28.cumsum())
</code></pre>
<p>输出：</p>
<pre><code>30
3.0
5
1
1.4142135623730951
2.0
[ 1  3  6 10 15 20 24 27 29 30]
</code></pre>
<p>####其他方法</p>
<ol>
<li><p><code>all()</code> / <code>any()</code>方法：判断数组是否所有元素都是<code>True</code> / 判断数组是否有为<code>True</code>的元素。</p>
</li>
<li><p><code>astype()</code>方法：拷贝数组，并将数组中的元素转换为指定的类型。</p>
</li>
<li><p><code>dot()</code>方法：实现一个数组和另一个数组的点积运算。</p>
<p> 在数学上，<strong>点积</strong>（dot product）又称<strong>数量积</strong>或<strong>标量积</strong>，是一种接受两个等长的数字序列，返回单个数字的代数运算。从代数角度看，先对两个数字序列中的每组对应元素求积，再对所有积求和，结果即为点积，即：$\boldsymbol{A} \cdot \boldsymbol{B} = \sum_{i=1}^{n}a_ib_i$。从几何角度看，点积则是两个向量的长度与它们夹角余弦的积，即：$\boldsymbol{A} \cdot \boldsymbol{B}=|\boldsymbol{A}||\boldsymbol{B}|\cos{\theta}$。</p>
<p> 在欧几里得几何中，两个笛卡尔坐标向量的点积也称为<strong>内积</strong>（inner product），NumPy中也提供了实现内积的函数，但是内积的含义要高于点积，点积相当于是内积在欧几里得空间$\mathbb{R}^n$的特例，而内积可以推广到<strong>赋范向量空间</strong>（不理解没有关系，当我没说就行了）。</p>
<p> 一维数组的点积运算，代码：</p>
<pre><code class="Python">array29 = np.array([3, 4])
array30 = np.array([5, 6])
array29.dot(array30)
</code></pre>
<p> 输出：</p>
<pre><code>39
</code></pre>
<p> 二维数组的点积运算，代码：</p>
<pre><code class="Python">array31 = np.array([[1, 2, 3], [4, 5, 6]])
array32 = np.array([[1, 2], [3, 4], [5, 6]])
array31.dot(array32)
</code></pre>
<p> 输出：</p>
<pre><code>array([[22, 28],
       [49, 64]])
</code></pre>
<blockquote>
<p><strong>说明</strong>：可以看出，二维数组的点积就是矩阵乘法运算。</p>
</blockquote>
</li>
<li><p><code>dump()</code>方法：保存数组到文件中，可以通过NumPy中的<code>load</code>函数从保存的文件中加载数据创建数组。</p>
<p> 代码：</p>
<pre><code class="Python">array31.dump(&#39;array31-data&#39;)
array32 = np.load(&#39;array31-data&#39;, allow_pickle=True)
array32
</code></pre>
<p> 输出：</p>
<pre><code>array([[1, 2],
       [3, 4],
       [5, 6]])
</code></pre>
</li>
<li><p><code>fill()</code>方法：向数组中填充指定的元素。</p>
</li>
<li><p><code>flatten()</code>方法：将多维数组扁平化为一维数组。</p>
<p> 代码：</p>
<pre><code class="Python">array32.flatten()
</code></pre>
<p> 输出：</p>
<pre><code>array([1, 2, 3, 4, 5, 6])
</code></pre>
</li>
<li><p><code>nonzero()</code>方法：返回非0元素的索引。</p>
</li>
<li><p><code>round()</code>方法：对数组中的元素做四舍五入操作。</p>
</li>
<li><p><code>sort()</code>方法：对数组进行就地排序。</p>
<p> 代码：</p>
<pre><code class="Python">array33 = np.array([35, 96, 12, 78, 66, 54, 40, 82])
array33.sort()
array33
</code></pre>
<p> 输出：</p>
<pre><code>array([12, 35, 40, 54, 66, 78, 82, 96])
</code></pre>
</li>
<li><p><code>swapaxes()</code>和<code>transpose()</code>方法：交换数组指定的轴。</p>
</li>
</ol>
<p>   代码：</p>
<pre><code class="Python"># 指定需要交换的两个轴，顺序无所谓
array32.swapaxes(0, 1)
</code></pre>
<p>   输出：</p>
<pre><code>array([[1, 3, 5],
       [2, 4, 6]])
</code></pre>
<p>   代码：</p>
<pre><code class="Python"># 对于二维数组，transpose相当于实现了矩阵的转置
array32.transpose()
</code></pre>
<p>   输出：</p>
<pre><code>array([[1, 3, 5],
       [2, 4, 6]])
</code></pre>
<ol start="11">
<li><p><code>take()</code>方法：从数组中取指定索引的元素，类似于花式索引。</p>
<p> 代码：</p>
<pre><code class="Python">array34 = array33.take([0, 2, -3, -1])
array34
</code></pre>
<p> 输出：</p>
<pre><code>array([12, 40, 78, 96])
</code></pre>
</li>
<li><p><code>tolist()</code>方法：将数组转成Python中的<code>list</code>。</p>
</li>
</ol>
<h3 id="数组的运算"><a href="#数组的运算" class="headerlink" title="数组的运算"></a>数组的运算</h3><p>使用NumPy最为方便的是当需要对数组元素进行运算时，不用编写循环代码遍历每个元素，所有的运算都会自动的<strong>矢量化</strong>（使用高效的提前编译的底层语言代码来对数据序列进行数学操作）。简单的说就是，NumPy中的数学运算和数学函数会自动作用于数组中的每个成员。</p>
<h4 id="数组跟标量的运算"><a href="#数组跟标量的运算" class="headerlink" title="数组跟标量的运算"></a>数组跟标量的运算</h4><p>代码：</p>
<pre><code class="Python">array35 = np.arange(1, 10)
print(array35 + 10)
print(array35 * 10)
</code></pre>
<p>输出：</p>
<pre><code>[11 12 13 14 15 16 17 18 19]
[10 20 30 40 50 60 70 80 90]
</code></pre>
<h4 id="数组跟数组的运算"><a href="#数组跟数组的运算" class="headerlink" title="数组跟数组的运算"></a>数组跟数组的运算</h4><p>代码：</p>
<pre><code class="Python">array36 = np.array([1, 1, 1, 2, 2, 2, 3, 3, 3])
print(array35 + array36)
print(array35 * array36)
print(array35 ** array36)
</code></pre>
<p>输出：</p>
<pre><code>[ 2  3  4  6  7  8 10 11 12]
[ 1  2  3  8 10 12 21 24 27]
[  1   2   3  16  25  36 343 512 729]
</code></pre>
<h4 id="通用一元函数"><a href="#通用一元函数" class="headerlink" title="通用一元函数"></a>通用一元函数</h4><p>通用函数是对<code>ndarray</code>中的数据执行元素级运算的函数。你可以将其看做普通函数（接收一个标量值作为参数，返回一个标量值）的矢量化包装器，如下所示。</p>
<p>代码：</p>
<pre><code class="Python">print(np.sqrt(array35))
print(np.log2(array35))
</code></pre>
<p>输出：</p>
<pre><code>[1.         1.41421356 1.73205081 2.         2.23606798 2.44948974
 2.64575131 2.82842712 3.        ]
[0.         1.         1.5849625  2.         2.32192809 2.5849625
 2.80735492 3.         3.169925  ]
</code></pre>
<p><strong>表1：通用一元函数</strong></p>
<table>
<thead>
<tr>
<th>函数</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td><code>abs</code> / <code>fabs</code></td>
<td>求绝对值的函数</td>
</tr>
<tr>
<td><code>sqrt</code></td>
<td>求平方根的函数，相当于<code>array ** 0.5 </code></td>
</tr>
<tr>
<td><code>square</code></td>
<td>求平方的函数，相当于<code>array ** 2</code></td>
</tr>
<tr>
<td><code>exp</code></td>
<td>计算$e^x$的函数</td>
</tr>
<tr>
<td><code>log</code> / <code>log10</code> / <code>log2</code></td>
<td>对数函数（<code>e</code>为底 / <code>10</code>为底 / <code>2</code>为底）</td>
</tr>
<tr>
<td><code>sign</code></td>
<td>符号函数（<code>1</code> - 正数；<code>0</code> - 零；<code>-1</code> - 负数）</td>
</tr>
<tr>
<td><code>ceil</code> / <code>floor</code></td>
<td>上取整 /  下取整</td>
</tr>
<tr>
<td><code>isnan</code></td>
<td>返回布尔数组，NaN对应<code>True</code>，非NaN对应<code>False</code></td>
</tr>
<tr>
<td><code>isfinite</code> / <code>isinf</code></td>
<td>判断数值是否为无穷大的函数</td>
</tr>
<tr>
<td><code>cos</code> / <code>cosh</code> / <code>sin</code></td>
<td>三角函数</td>
</tr>
<tr>
<td><code>sinh</code> / <code>tan</code> / <code>tanh</code></td>
<td>三角函数</td>
</tr>
<tr>
<td><code>arccos</code> / <code>arccosh</code> / <code>arcsin</code></td>
<td>反三角函数</td>
</tr>
<tr>
<td><code>arcsinh</code> / <code>arctan</code> / <code>arctanh</code></td>
<td>反三角函数</td>
</tr>
<tr>
<td><code>rint</code> / <code>round</code></td>
<td>四舍五入函数</td>
</tr>
</tbody></table>
<h4 id="通用二元函数"><a href="#通用二元函数" class="headerlink" title="通用二元函数"></a>通用二元函数</h4><p>代码：</p>
<pre><code class="Python">array37 = np.array([[4, 5, 6], [7, 8, 9]])
array38 = np.array([[1, 2, 3], [3, 2, 1]])
print(array37 * array38)
print(np.power(array37, array38))
</code></pre>
<p>输出：</p>
<pre><code>[[ 4 10 18]
 [21 16  9]]
[[  4  25 216]
 [343  64   9]]
</code></pre>
<p><strong>表2：通用二元函数</strong></p>
<table>
<thead>
<tr>
<th>函数</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td><code>add(x, y)</code> / <code>substract(x, y)</code></td>
<td>加法函数 / 减法函数</td>
</tr>
<tr>
<td><code>multiply(x, y)</code> / <code>divide(x, y)</code></td>
<td>乘法函数 / 除法函数</td>
</tr>
<tr>
<td><code>floor_divide(x, y)</code> / <code>mod(x, y)</code></td>
<td>整除函数 / 求模函数</td>
</tr>
<tr>
<td><code>allclose(x, y)</code></td>
<td>检查数组<code>x</code>和<code>y</code>元素是否几乎相等</td>
</tr>
<tr>
<td><code>power(x, y)</code></td>
<td>数组$x$的元素$x_i$和数组$y$的元素$y_i$，计算$x_i^{y_i}$</td>
</tr>
<tr>
<td><code>maximum(x, y)</code> / <code>fmax(x, y)</code></td>
<td>两两比较元素获取最大值 / 获取最大值（忽略NaN）</td>
</tr>
<tr>
<td><code>minimum(x, y)</code> / <code>fmin(x, y)</code></td>
<td>两两比较元素获取最小值 / 获取最小值（忽略NaN）</td>
</tr>
<tr>
<td><code>inner(x, y)</code></td>
<td>内积运算</td>
</tr>
<tr>
<td><code>cross(x, y) </code>/ <code>outer(x, y)</code></td>
<td>叉积运算 / 外积运算</td>
</tr>
<tr>
<td><code>intersect1d(x, y)</code></td>
<td>计算<code>x</code>和<code>y</code>的交集，返回这些元素构成的有序数组</td>
</tr>
<tr>
<td><code>union1d(x, y)</code></td>
<td>计算<code>x</code>和<code>y</code>的并集，返回这些元素构成的有序数组</td>
</tr>
<tr>
<td><code>in1d(x, y)</code></td>
<td>返回由判断<code>x</code> 的元素是否在<code>y</code>中得到的布尔值构成的数组</td>
</tr>
<tr>
<td><code>setdiff1d(x, y)</code></td>
<td>计算<code>x</code>和<code>y</code>的差集，返回这些元素构成的数组</td>
</tr>
<tr>
<td><code>setxor1d(x, y)</code></td>
<td>计算<code>x</code>和<code>y</code>的对称差，返回这些元素构成的数组</td>
</tr>
</tbody></table>
<blockquote>
<p><strong>补充说明</strong>：在二维空间内，两个向量$\boldsymbol{A}=\begin{bmatrix} a_1 \ a_2 \end{bmatrix}$和$\boldsymbol{B}=\begin{bmatrix} b_1 \ b_2 \end{bmatrix}$的叉积是这样定义的：$\boldsymbol{A}\times \boldsymbol{B}=\begin{vmatrix} a_1 \quad a_2 \ b_1 \quad b_2 \end{vmatrix}=a_1b_2 - a_2b_1$，其中$\begin{vmatrix} a_1 \quad a_2 \ b_1 \quad b_2 \end{vmatrix}$称为行列式。但是一定要注意，叉积并不等同于行列式，行列式的运算结果是一个标量，而叉积运算的结果是一个向量。如果不明白，我们可以看看三维空间两个向量，$\boldsymbol{A}=\begin{bmatrix} a_1 \ a_2 \ a_3 \end{bmatrix}$和$\boldsymbol{B}=\begin{bmatrix} b_1 \ b_2 \ b_3 \end{bmatrix}$的叉积是$\left&lt; \hat{i} \begin{vmatrix} a_2 \quad a_3 \ b_2 \quad b_3 \end{vmatrix}, -\hat{j} \begin{vmatrix} a_1 \quad a_3 \ b_1 \quad b_3 \end{vmatrix}, \hat{k} \begin{vmatrix} a_1 \quad a_2 \ b_1 \quad b_2 \end{vmatrix} \right&gt;$，其中$\hat{i}, \hat{j}, \hat{k}$代表每个维度的单位向量。</p>
</blockquote>
<h4 id="广播机制"><a href="#广播机制" class="headerlink" title="广播机制"></a>广播机制</h4><p>上面的例子中，两个二元运算的数组形状是完全相同的，我们再来研究一下，两个形状不同的数组是否可以直接做二元运算或使用二元函数进行运算，请看下面的例子。</p>
<p>代码：</p>
<pre><code class="Python">array39 = np.array([[0, 0, 0], [1, 1, 1], [2, 2, 2], [3, 3, 3]])
array40 = np.array([1, 2, 3])
array39 + array40
</code></pre>
<p>输出：</p>
<pre><code>array([[1, 2, 3],
       [2, 3, 4],
       [3, 4, 5],
       [4, 5, 6]])
</code></pre>
<p>代码：</p>
<pre><code class="Python">array41 = np.array([[1], [2], [3], [4]])
array39 + array41
</code></pre>
<p>输出：</p>
<pre><code>array([[1, 1, 1],
       [3, 3, 3],
       [5, 5, 5],
       [7, 7, 7]])
</code></pre>
<p>通过上面的例子，我们发现形状不同的数组仍然有机会进行二元运算，但也绝对不是任意的数组都可以进行二元运算。简单的说，只有两个数组后缘维度相同或者其中一个数组后缘维度为1时，广播机制会被触发，而通过广播机制如果能够使两个数组的形状一致，才能进行二元运算。所谓后缘维度，指的是数组<code>shape</code>属性对应的元组中最后一个元素的值（从后往前数最后一个维度的值），例如，我们之前打开的图像对应的数组后缘维度为3，3行4列的二维数组后缘维度为4，而有5个元素的一维数组后缘维度为5。简单的说就是，后缘维度相同或者其中一个数组的后缘维度为1，就可以应用广播机制；而广播机制如果能够使得数组的形状一致，就满足了两个数组对应元素做运算的需求，如下图所示。</p>
<p><img src="res/broadcast-1.png"></p>
<p><img src="res/broadcast-2.png"></p>
<p><img src="res/broadcast-3.png"></p>
<h3 id="其他常用函数"><a href="#其他常用函数" class="headerlink" title="其他常用函数"></a>其他常用函数</h3><p>除了上面讲到的函数外，NumPy中还提供了很多用于处理数组的函数，<code>ndarray</code>对象的很多方法也可以通过直接调用函数来实现，下表给出了一些常用的函数。</p>
<p><strong>表3：NumPy其他常用函数</strong></p>
<table>
<thead>
<tr>
<th>函数</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td><code>unique</code></td>
<td>去除数组重复元素，返回唯一元素构成的有序数组</td>
</tr>
<tr>
<td><code>copy</code></td>
<td>返回拷贝数组得到的数组</td>
</tr>
<tr>
<td><code>sort</code></td>
<td>返回数组元素排序后的拷贝</td>
</tr>
<tr>
<td><code>split</code> / <code>hsplit</code> / <code>vsplit</code></td>
<td>将数组拆成若干个子数组</td>
</tr>
<tr>
<td><code>stack</code> / <code>hstack</code> / <code>vstack</code></td>
<td>将多个数组堆叠成新数组</td>
</tr>
<tr>
<td><code>concatenate</code></td>
<td>沿着指定的轴连接多个数组构成新数组</td>
</tr>
<tr>
<td><code>append</code> / <code>insert</code></td>
<td>向数组末尾追加元素 / 在数组指定位置插入元素</td>
</tr>
<tr>
<td><code>argwhere</code></td>
<td>找出数组中非0元素的位置</td>
</tr>
<tr>
<td><code>extract</code> / <code>select</code> / <code>where</code></td>
<td>按照指定的条件从数组中抽取或处理数组元素</td>
</tr>
<tr>
<td><code>flip</code></td>
<td>沿指定的轴翻转数组中的元素</td>
</tr>
<tr>
<td><code>fromiter</code></td>
<td>通过迭代器创建数组对象</td>
</tr>
<tr>
<td><code>fromregex</code></td>
<td>通过读取文件和正则表达式解析获取数据创建数组对象</td>
</tr>
<tr>
<td><code>repeat</code> / <code>tile</code></td>
<td>通过对元素的重复来创建新数组</td>
</tr>
<tr>
<td><code>roll</code></td>
<td>沿指定轴对数组元素进行移位</td>
</tr>
<tr>
<td><code>resize</code></td>
<td>重新调整数组的大小</td>
</tr>
<tr>
<td><code>place</code> / <code>put</code></td>
<td>将数组中满足条件的元素/指定的元素替换为指定的值</td>
</tr>
<tr>
<td><code>ptp</code></td>
<td>沿指定的轴计算极差（最大值与最小值的差）</td>
</tr>
<tr>
<td><code>median</code></td>
<td>沿指定轴计算中位数</td>
</tr>
<tr>
<td><code>partition</code></td>
<td>用选定的元素对数组进行一次划分并返回划分后的数组</td>
</tr>
</tbody></table>
<blockquote>
<p><strong>提示</strong>：上面的<code>resize</code>函数和<code>ndarray</code>对象的<code>resize</code>方法是有区别的，<code>resize</code>函数在调整数组大小时会重复数组中的元素作为填补多出来的元素的值，而<code>ndarry</code>对象的<code>resize</code>方法是用0来填补多出来的元素。这些小细节不清楚暂时也不要紧，但是如果用到对应的功能了就要引起注意。</p>
</blockquote>
<p>代码：</p>
<pre><code class="Python">array42 = np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]])
array43 = np.array([[4, 4, 4], [5, 5, 5], [6, 6, 6]])
np.hstack((array42, array43))
</code></pre>
<p>输出：</p>
<pre><code>array([[1, 1, 1, 4, 4, 4],
       [2, 2, 2, 5, 5, 5],
       [3, 3, 3, 6, 6, 6]])
</code></pre>
<p>代码：</p>
<pre><code class="Python">np.vstack((array42, array43))
</code></pre>
<p>输出：</p>
<pre><code>array([[1, 1, 1],
       [2, 2, 2],
       [3, 3, 3],
       [4, 4, 4],
       [5, 5, 5],
       [6, 6, 6]])
</code></pre>
<p>代码：</p>
<pre><code class="Python">np.concatenate((array42, array43))
</code></pre>
<p>输出：</p>
<pre><code>array([[1, 1, 1],
       [2, 2, 2],
       [3, 3, 3],
       [4, 4, 4],
       [5, 5, 5],
       [6, 6, 6]])
</code></pre>
<p>代码：</p>
<pre><code class="Python">np.concatenate((array42, array43), axis=1)
</code></pre>
<p>输出：</p>
<pre><code>array([[1, 1, 1, 4, 4, 4],
       [2, 2, 2, 5, 5, 5],
       [3, 3, 3, 6, 6, 6]])
</code></pre>
<h3 id="矩阵运算"><a href="#矩阵运算" class="headerlink" title="矩阵运算"></a>矩阵运算</h3><p>NumPy中提供了专门用于线性代数（linear algebra）的模块和表示矩阵的类型<code>matrix</code>，当然我们通过二维数组也可以表示一个矩阵，官方并不推荐使用<code>matrix</code>类而是建议使用二维数组，而且有可能在将来的版本中会移除<code>matrix</code>类。无论如何，利用这些已经封装好的类和函数，我们可以轻松愉快的实现线性代数中很多的操作。</p>
<h4 id="线性代数快速回顾"><a href="#线性代数快速回顾" class="headerlink" title="线性代数快速回顾"></a>线性代数快速回顾</h4><ol>
<li><strong>向量</strong>也叫<strong>矢量</strong>，是一个同时具有大小和方向，且满足平行四边形法则的几何对象。与向量相对的概念叫<strong>标量</strong>或<strong>数量</strong>，标量只有大小、绝大多数情况下没有方向。</li>
<li>向量可以进行<strong>加</strong>、<strong>减</strong>、<strong>数乘</strong>、<strong>点积</strong>、<strong>叉积</strong>等运算。</li>
<li><strong>行列式</strong>由向量组成，它的性质可以由向量解释。</li>
<li>行列式可以使用<strong>行列式公式</strong>计算：$det(\boldsymbol{A})=\sum_{n!} \pm {a_{1\alpha}a_{2\beta} \cdots a_{n\omega}}$。</li>
<li>高阶行列式可以用<strong>代数余子式</strong>展开成多个低阶行列式，如：$det(\boldsymbol{A})=a_{11}C_{11}+a_{12}C_{12}+ \cdots +a_{1n}C_{1n}$。</li>
<li><strong>矩阵</strong>是由一系列元素排成的矩形阵列，矩阵里的元素可以是数字、符号或数学公式。</li>
<li>矩阵可以进行<strong>加法</strong>、<strong>减法</strong>、<strong>数乘</strong>、<strong>乘法</strong>、<strong>转置</strong>等运算。</li>
<li><strong>逆矩阵</strong>用$\boldsymbol{A^{-1}}$表示，$\boldsymbol{A}\boldsymbol{A^{-1}}=\boldsymbol{A^{-1}}\boldsymbol{A}=\boldsymbol{I}$；没有逆矩阵的方阵是<strong>奇异矩阵</strong>。</li>
<li>如果一个方阵是<strong>满秩矩阵</strong>(矩阵的秩等于矩阵的阶数)，该方阵对应的线性方程有唯一解。</li>
</ol>
<blockquote>
<p><strong>说明</strong>：<strong>矩阵的秩</strong>是指矩阵中线性无关的行/列向量的最大个数，同时也是矩阵对应的线性变换的像空间的维度。</p>
</blockquote>
<h4 id="NumPy中矩阵相关函数"><a href="#NumPy中矩阵相关函数" class="headerlink" title="NumPy中矩阵相关函数"></a>NumPy中矩阵相关函数</h4><ol>
<li><p>创建矩阵对象。</p>
<p> 代码：</p>
<pre><code class="Python"># matrix构造函数可以传入类数组对象也可以传入字符串
m1 = np.matrix(&#39;1 2 3; 4 5 6&#39;)
m1
</code></pre>
<p> 输出：</p>
<pre><code>matrix([[1, 2, 3],
        [4, 5, 6]])
</code></pre>
<p> 代码：</p>
<pre><code class="Python"># asmatrix函数也可以写成mat函数，它们其实是同一个函数
m2 = np.asmatrix(np.array([[1, 1], [2, 2], [3, 3]]))
m2
</code></pre>
<p> 输出：</p>
<pre><code>matrix([[1, 1],
        [2, 2],
        [3, 3]])
</code></pre>
<p> 代码：</p>
<pre><code class="Python">m1 * m2
</code></pre>
<p> 输出：</p>
<pre><code>matrix([[14, 14],
        [32, 32]])
</code></pre>
<blockquote>
<p><strong>说明</strong>：注意<code>matrix</code>对象和<code>ndarray</code>对象乘法运算的差别，如果两个二维数组要做矩阵乘法运算，应该使用<code>@</code>运算符或<code>matmul</code>函数，而不是<code>*</code>运算符。</p>
</blockquote>
</li>
<li><p>矩阵对象的属性。</p>
<table>
<thead>
<tr>
<th>属性</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td><code>A</code></td>
<td>获取矩阵对象对应的<code>ndarray</code>对象</td>
</tr>
<tr>
<td><code>A1</code></td>
<td>获取矩阵对象对应的扁平化后的<code>ndarray</code>对象</td>
</tr>
<tr>
<td><code>I</code></td>
<td>可逆矩阵的逆矩阵</td>
</tr>
<tr>
<td><code>T</code></td>
<td>矩阵的转置</td>
</tr>
<tr>
<td><code>H</code></td>
<td>矩阵的共轭转置</td>
</tr>
<tr>
<td><code>shape</code></td>
<td>矩阵的形状</td>
</tr>
<tr>
<td><code>size</code></td>
<td>矩阵元素的个数</td>
</tr>
</tbody></table>
</li>
<li><p>矩阵对象的方法。</p>
</li>
</ol>
<p>  矩阵对象的方法跟之前讲过的<code>ndarray</code>数组对象的方法基本差不多，此处不再进行赘述。</p>
<h4 id="NumPy的线性代数模块"><a href="#NumPy的线性代数模块" class="headerlink" title="NumPy的线性代数模块"></a>NumPy的线性代数模块</h4><p>NumPy的<code>linalg</code>模块中有一组标准的矩阵分解运算以及诸如求逆和行列式之类的函数，它们跟MATLAB和R等语言所使用的是相同的行业标准线性代数库，下面的表格列出了<code>numpy</code>以及<code>linalg</code>模块中常用的跟线性代数相关的函数。</p>
<table>
<thead>
<tr>
<th>函数</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td><code>diag</code></td>
<td>以一维数组的形式返回方阵的对角线元素或将一维数组转换为方阵（非对角元素元素为0）</td>
</tr>
<tr>
<td><code>vdot</code></td>
<td>向量的点积</td>
</tr>
<tr>
<td><code>dot</code></td>
<td>数组的点积</td>
</tr>
<tr>
<td><code>inner</code></td>
<td>数组的内积</td>
</tr>
<tr>
<td><code>outer</code></td>
<td>数组的叉积</td>
</tr>
<tr>
<td><code>trace</code></td>
<td>计算对角线元素的和</td>
</tr>
<tr>
<td><code>norm</code></td>
<td>求模运算</td>
</tr>
<tr>
<td><code>det</code></td>
<td>计算行列式的值（在方阵上计算得到的标量）</td>
</tr>
<tr>
<td><code>matrix_rank</code></td>
<td>计算矩阵的秩</td>
</tr>
<tr>
<td><code>eig</code></td>
<td>计算矩阵的特征值（eigenvalue）和特征向量（eigenvector）</td>
</tr>
<tr>
<td><code>inv</code></td>
<td>计算非奇异矩阵（$n$阶方阵）的逆矩阵</td>
</tr>
<tr>
<td><code>pinv</code></td>
<td>计算矩阵的摩尔-彭若斯（Moore-Penrose）广义逆</td>
</tr>
<tr>
<td><code>qr</code></td>
<td>QR分解（把矩阵分解成一个正交矩阵与一个上三角矩阵的积）</td>
</tr>
<tr>
<td><code>svd</code></td>
<td>计算奇异值分解（singular value decomposition）</td>
</tr>
<tr>
<td><code>solve</code></td>
<td>解线性方程组$\boldsymbol{A}\boldsymbol{x}=\boldsymbol{b}$，其中$\boldsymbol{A}$是一个方阵</td>
</tr>
<tr>
<td><code>lstsq</code></td>
<td>计算$\boldsymbol{A}\boldsymbol{x}=\boldsymbol{b}$的最小二乘解</td>
</tr>
</tbody></table>
<p>大家如果有兴趣可以用下面的代码验证上面的函数。</p>
<p>代码：</p>
<pre><code class="Python">m3 = np.array([[1., 2.], [3., 4.]])
np.linalg.inv(m3)
</code></pre>
<p>输出：</p>
<pre><code>array([[-2. ,  1. ],
       [ 1.5, -0.5]])
</code></pre>
<p>代码：</p>
<pre><code class="Python">m4 = np.array([[1, 3, 5], [2, 4, 6], [4, 7, 9]])
np.linalg.det(m4)
</code></pre>
<p>输出：</p>
<pre><code>2
</code></pre>
<p>代码：</p>
<pre><code class="Python"># 解线性方程组ax=b
# 3x + y = 9，x + 2y = 8
a = np.array([[3,1], [1,2]])
b = np.array([9, 8])
np.linalg.solve(a, b)
</code></pre>
<p>输出：</p>
<pre><code>array([2., 3.])
</code></pre>

        
        
          <blockquote id="copyright">
              <p>Original link: <a href="http://example.com/page/6/index.html">http://example.com/page/6/index.html</a></p>
              <p>Copyright Notice: 转载请注明出处.</p>
          </blockquote>
        
      
    </div>
    <footer class="article-footer">
      
        <div class="article-tag-wrap">
          

          
          
    <div class="social-share">
      <span>Share:</span>
    </div>



        </div>
      
      
      
    </footer>
  </div>
</article>


  <article id="post-Python-100-Days-master/Day61-65/65.解析动态内容" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-inner">
    
    
    <div class="article-meta">
      
      <span class="article-date">
  <i class="fa fa-date"></i>
  <time class="dt-published" datetime="2021-10-17T13:24:55.580Z" itemprop="datePublished">2021年10月17日</time>
</span>
      
      
      
<a href="/2021/10/17/Python-100-Days-master/Day61-65/65.%E8%A7%A3%E6%9E%90%E5%8A%A8%E6%80%81%E5%86%85%E5%AE%B9/#comments" class="article-comment-link">
  
  <i class="fa fa-commt"></i>
  Guestbook
</a>


    </div>
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="解析动态内容"><a href="#解析动态内容" class="headerlink" title="解析动态内容"></a>解析动态内容</h2><p>根据权威机构发布的全球互联网可访问性审计报告，全球约有四分之三的网站其内容或部分内容是通过JavaScript动态生成的，这就意味着在浏览器窗口中“查看网页源代码”时无法在HTML代码中找到这些内容，也就是说我们之前用的抓取数据的方式无法正常运转了。解决这样的问题基本上有两种方案，一是JavaScript逆向工程；另一种是渲染JavaScript获得渲染后的内容。</p>
<h3 id="JavaScript逆向工程"><a href="#JavaScript逆向工程" class="headerlink" title="JavaScript逆向工程"></a>JavaScript逆向工程</h3><p>下面我们以“360图片”网站为例，说明什么是JavaScript逆向工程。其实所谓的JavaScript逆向工程就是找到通过Ajax技术动态获取数据的接口。在浏览器中输入<a target="_blank" rel="noopener" href="http://image.so.com/z?ch=beauty">http://image.so.com/z?ch=beauty</a>就可以打开“360图片”的“美女”版块，如下图所示。</p>
<p><img src="./res/image360-website.png"></p>
<p>但是当我们在浏览器中通过右键菜单“显示网页源代码”的时候，居然惊奇的发现页面的HTML代码中连一个<code>&lt;img&gt;</code>标签都没有，那么我们看到的图片是怎么显示出来的呢？原来所有的图片都是通过JavaScript动态加载的，而在浏览器的“开发人员工具”的“网络”中可以找到获取这些图片数据的网络API接口，如下图所示。</p>
<p><img src="./res/api-image360.png"></p>
<p>那么结论就很简单了，只要我们找到了这些网络API接口，那么就能通过这些接口获取到数据，当然实际开发的时候可能还要对这些接口的参数以及接口返回的数据进行分析，了解每个参数的意义以及返回的JSON数据的格式，这样才能在我们的爬虫中使用这些数据。</p>
<h3 id="使用Selenium"><a href="#使用Selenium" class="headerlink" title="使用Selenium"></a>使用Selenium</h3><p>尽管很多网站对自己的网络API接口进行了保护，增加了获取数据的难度，但是只要经过足够的努力，绝大多数还是可以被逆向工程的，但是在实际开发中，我们可以通过浏览器渲染引擎来避免这些繁琐的工作，WebKit就是一个利用的渲染引擎。</p>
<p>WebKit的代码始于1998年的KHTML项目，当时它是Konqueror浏览器的渲染引擎。2001年，苹果公司从这个项目的代码中衍生出了WebKit并应用于Safari浏览器，早期的Chrome浏览器也使用了该内核。在Python中，我们可以通过Qt框架获得WebKit引擎并使用它来渲染页面获得动态内容，关于这个内容请大家自行阅读<a target="_blank" rel="noopener" href="http://python.jobbole.com/84600/">《爬虫技术:动态页面抓取超级指南》</a>一文。</p>
<p>如果没有打算用上面所说的方式来渲染页面并获得动态内容，其实还有一种替代方案就是使用自动化测试工具Selenium，它提供了浏览器自动化的API接口，这样就可以通过操控浏览器来获取动态内容。首先可以使用pip来安装Selenium。</p>
<pre><code class="Shell">pip3 install selenium
</code></pre>
<p>下面以“阿里V任务”的“直播服务”为例，来演示如何使用Selenium获取到动态内容并抓取主播图片。</p>
<pre><code class="Python">import requests

from bs4 import BeautifulSoup


def main():
    resp = requests.get(&#39;https://v.taobao.com/v/content/live?catetype=704&amp;from=taonvlang&#39;)
    soup = BeautifulSoup(resp.text, &#39;lxml&#39;)
    for img_tag in soup.select(&#39;img[src]&#39;):
        print(img_tag.attrs[&#39;src&#39;])


if __name__ == &#39;__main__&#39;:
    main()
</code></pre>
<p>运行上面的程序会发现没有任何的输出，因为页面的HTML代码上根本找不到<code>&lt;img&gt;</code>标签。接下来我们使用Selenium来获取到页面上的动态内容，再提取主播图片。</p>
<pre><code class="Python">from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.keys import Keys


def main():
    driver = webdriver.Chrome()
    driver.get(&#39;https://v.taobao.com/v/content/live?catetype=704&amp;from=taonvlang&#39;)
    soup = BeautifulSoup(driver.page_source, &#39;lxml&#39;)
    for img_tag in soup.body.select(&#39;img[src]&#39;):
        print(img_tag.attrs[&#39;src&#39;])


if __name__ == &#39;__main__&#39;:
    main()
</code></pre>
<p>在上面的程序中，我们通过Selenium实现对Chrome浏览器的操控，如果要操控其他的浏览器，可以创对应的浏览器对象，例如Firefox、IE等。运行上面的程序，如果看到如下所示的错误提示，那是说明我们还没有将Chrome浏览器的驱动添加到PATH环境变量中，也没有在程序中指定Chrome浏览器驱动所在的位置。</p>
<pre><code class="Shell">selenium.common.exceptions.WebDriverException: Message: &#39;chromedriver&#39; executable needs to be in PATH. Please see https://sites.google.com/a/chromium.org/chromedriver/home
</code></pre>
<p>为了解决上面的问题，可以到Selenium的<a target="_blank" rel="noopener" href="https://www.seleniumhq.org/">官方网站</a>找到浏览器驱动的下载链接并下载需要的驱动，在Linux或macOS系统下可以通过下面的命令来设置PATH环境变量，Windows下配置环境变量也非常简单，不清楚的可以自行了解。</p>
<pre><code class="Shell">export PATH=$PATH:/Users/Hao/Downloads/Tools/chromedriver/
</code></pre>
<p>其中<code>/Users/Hao/Downloads/Tools/chromedriver/ </code>就是chromedriver所在的路径。当然，更为简单的办法是把chromedriver直接放在虚拟环境中，跟Python解释器位于同一个路径下就可以了。</p>
<h3 id="WebDriver用法详解"><a href="#WebDriver用法详解" class="headerlink" title="WebDriver用法详解"></a>WebDriver用法详解</h3><p>表1. 定位页面元素的方法</p>
<p>表2. WebDriver的常用属性</p>
<table>
<thead>
<tr>
<th>属性</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>current_url</td>
<td>当前页面的URL</td>
</tr>
<tr>
<td>current_window_handle</td>
<td>当前窗口的句柄（引用）</td>
</tr>
<tr>
<td>name</td>
<td>WebDriver实例底层浏览器的名称</td>
</tr>
<tr>
<td>orientation</td>
<td>当前设备的方向（横屏、竖屏）</td>
</tr>
<tr>
<td>page_source</td>
<td>当前页面的源代码（动态内容）</td>
</tr>
<tr>
<td>title</td>
<td>当前页面的标题</td>
</tr>
<tr>
<td>window_handles</td>
<td>WebDriver打开的所有窗口的句柄</td>
</tr>
</tbody></table>
<p>表3. WebDriver的常用方法</p>
<table>
<thead>
<tr>
<th>方法</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>back() / forward()</td>
<td>在浏览历史记录中后退/前进</td>
</tr>
<tr>
<td>close() / quit()</td>
<td>关闭当前浏览器窗口 / 退出WebDriver实例</td>
</tr>
<tr>
<td>get(url)</td>
<td>加载指定URL的页面到浏览器中</td>
</tr>
<tr>
<td>maximize_window()</td>
<td>将浏览器窗口最大化</td>
</tr>
<tr>
<td>refresh()</td>
<td>刷新当前页面</td>
</tr>
<tr>
<td>switch_to_active_element()</td>
<td>获得页面上得到焦点的元素</td>
</tr>
<tr>
<td>switch_to_alert()</td>
<td>把焦点切换至弹出的警告框</td>
</tr>
<tr>
<td>set_page_load_timeout(time_to_wait)</td>
<td>设置页面加载超时时间</td>
</tr>
<tr>
<td>set_script_timeout(time_to_wait)</td>
<td>设置JavaScript执行超时时间</td>
</tr>
<tr>
<td>implicit_wait(time_to_wait)</td>
<td>设置等待元素被找到或目标指令完成</td>
</tr>
</tbody></table>
<h3 id="WebElement用法"><a href="#WebElement用法" class="headerlink" title="WebElement用法"></a>WebElement用法</h3><p>表1. WebElement常用属性</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<p>表2. WebElement常用方法</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<h3 id="Select用法"><a href="#Select用法" class="headerlink" title="Select用法"></a>Select用法</h3><h3 id="Alert用法"><a href="#Alert用法" class="headerlink" title="Alert用法"></a>Alert用法</h3><h3 id="元素等待机制"><a href="#元素等待机制" class="headerlink" title="元素等待机制"></a>元素等待机制</h3><h4 id="隐式等待"><a href="#隐式等待" class="headerlink" title="隐式等待"></a>隐式等待</h4><h4 id="显示等待"><a href="#显示等待" class="headerlink" title="显示等待"></a>显示等待</h4><h3 id="高级特性"><a href="#高级特性" class="headerlink" title="高级特性"></a>高级特性</h3><h4 id="鼠标和键盘事件"><a href="#鼠标和键盘事件" class="headerlink" title="鼠标和键盘事件"></a>鼠标和键盘事件</h4><h4 id="调用JavaScript"><a href="#调用JavaScript" class="headerlink" title="调用JavaScript"></a>调用JavaScript</h4><h4 id="屏幕截图和录制"><a href="#屏幕截图和录制" class="headerlink" title="屏幕截图和录制"></a>屏幕截图和录制</h4><h4 id="操作Cookie"><a href="#操作Cookie" class="headerlink" title="操作Cookie"></a>操作Cookie</h4>
        
        
          <blockquote id="copyright">
              <p>Original link: <a href="http://example.com/page/6/index.html">http://example.com/page/6/index.html</a></p>
              <p>Copyright Notice: 转载请注明出处.</p>
          </blockquote>
        
      
    </div>
    <footer class="article-footer">
      
        <div class="article-tag-wrap">
          

          
          
    <div class="social-share">
      <span>Share:</span>
    </div>



        </div>
      
      
      
    </footer>
  </div>
</article>


  <article id="post-Python-100-Days-master/Day61-65/64.并发下载" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-inner">
    
    
    <div class="article-meta">
      
      <span class="article-date">
  <i class="fa fa-date"></i>
  <time class="dt-published" datetime="2021-10-17T13:24:55.578Z" itemprop="datePublished">2021年10月17日</time>
</span>
      
      
      
<a href="/2021/10/17/Python-100-Days-master/Day61-65/64.%E5%B9%B6%E5%8F%91%E4%B8%8B%E8%BD%BD/#comments" class="article-comment-link">
  
  <i class="fa fa-commt"></i>
  Guestbook
</a>


    </div>
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="并发下载"><a href="#并发下载" class="headerlink" title="并发下载"></a>并发下载</h2><h3 id="多线程和多进程补充知识点"><a href="#多线程和多进程补充知识点" class="headerlink" title="多线程和多进程补充知识点"></a>多线程和多进程补充知识点</h3><h4 id="threading-local类"><a href="#threading-local类" class="headerlink" title="threading.local类"></a>threading.local类</h4><p>使用线程时最不愿意遇到的情况就是多个线程竞争资源，在这种情况下为了保证资源状态的正确性，我们可能需要对资源进行加锁保护的处理，这一方面会导致程序失去并发性，另外如果多个线程竞争多个资源时，还有可能因为加锁方式的不当导致<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E6%AD%BB%E9%94%81">死锁</a>。要解决多个线程竞争资源的问题，其中一个方案就是让每个线程都持有资源的副本（拷贝），这样每个线程可以操作自己所持有的资源，从而规避对资源的竞争。</p>
<p>要实现将资源和持有资源的线程进行绑定的操作，最简单的做法就是使用<code>threading</code>模块的<code>local</code>类，在网络爬虫开发中，就可以使用<code>local</code>类为每个线程绑定一个MySQL数据库连接或Redis客户端对象，这样通过线程可以直接获得这些资源，既解决了资源竞争的问题，又避免了在函数和方法调用时传递这些资源。具体的请参考本章多线程爬取“手机搜狐网”（Redis版）的实例代码。</p>
<h4 id="concurrent-futures模块"><a href="#concurrent-futures模块" class="headerlink" title="concurrent.futures模块"></a>concurrent.futures模块</h4><p>Python3.2带来了<code>concurrent.futures</code> 模块，这个模块包含了线程池和进程池、管理并行编程任务、处理非确定性的执行流程、进程/线程同步等功能。关于这部分的内容推荐大家阅读<a target="_blank" rel="noopener" href="http://python-parallel-programmning-cookbook.readthedocs.io/zh_CN/latest/index.html">《Python并行编程》</a>。</p>
<h4 id="分布式进程"><a href="#分布式进程" class="headerlink" title="分布式进程"></a>分布式进程</h4><p>使用多进程的时候，可以将进程部署在多个主机节点上，Python的<code>multiprocessing</code>模块不但支持多进程，其中<code>managers</code>子模块还支持把多进程部署到多个节点上。当然，要部署分布式进程，首先需要一个服务进程作为调度者，进程之间通过网络进行通信来实现对进程的控制和调度，由于<code>managers</code>模块已经对这些做出了很好的封装，因此在无需了解网络通信细节的前提下，就可以编写分布式多进程应用。具体的请参照本章分布式多进程爬取“手机搜狐网”的实例代码。</p>
<h3 id="协程和异步I-O"><a href="#协程和异步I-O" class="headerlink" title="协程和异步I/O"></a>协程和异步I/O</h3><h4 id="协程的概念"><a href="#协程的概念" class="headerlink" title="协程的概念"></a>协程的概念</h4><p>协程（coroutine）通常又称之为微线程或纤程，它是相互协作的一组子程序（函数）。所谓相互协作指的是在执行函数A时，可以随时中断去执行函数B，然后又中断继续执行函数A。注意，这一过程并不是函数调用（因为没有调用语句），整个过程看似像多线程，然而协程只有一个线程执行。协程通过<code>yield</code>关键字和 <code>send()</code>操作来转移执行权，协程之间不是调用者与被调用者的关系。</p>
<p>协程的优势在于以下两点：</p>
<ol>
<li>执行效率极高，因为子程序（函数）切换不是线程切换，由程序自身控制，没有切换线程的开销。</li>
<li>不需要多线程的锁机制，因为只有一个线程，也不存在竞争资源的问题，当然也就不需要对资源加锁保护，因此执行效率高很多。</li>
</ol>
<blockquote>
<p><strong>说明</strong>：协程适合处理的是I/O密集型任务，处理CPU密集型任务并不是它擅长的，如果要提升CPU的利用率可以考虑“多进程+多线程”或者“多进程+协程”的工作模式。</p>
</blockquote>
<h4 id="历史回顾"><a href="#历史回顾" class="headerlink" title="历史回顾"></a>历史回顾</h4><ol>
<li>Python 2.2：第一次提出了生成器（最初称之为迭代器）的概念（PEP 255）。</li>
<li>Python 2.5：引入了将对象发送回暂停了的生成器这一特性即生成器的<code>send()</code>方法（PEP 342）。</li>
<li>Python 3.3：添加了<code>yield from</code>特性，允许从迭代器中返回任何值（注意生成器本身也是迭代器），这样我们就可以串联生成器并且重构出更好的生成器。</li>
<li>Python 3.4：引入<code>asyncio.coroutine</code>装饰器用来标记作为协程的函数，协程函数和<code>asyncio</code>及其事件循环一起使用，来实现异步I/O操作。</li>
<li>Python 3.5：引入了<code>async</code>和<code>await</code>，可以使用<code>async def</code>来定义一个协程函数，这个函数中不能包含任何形式的<code>yield</code>语句，但是可以使用<code>return</code>或<code>await</code>从协程中返回值。</li>
</ol>
<p>协程实现了协作式并发，通过提高CPU的利用率来达到改善性能的目的。著名的三方库<a target="_blank" rel="noopener" href="https://github.com/aio-libs/aiohttp"><code>aiohttp</code></a>就是通过协程的方式实现了HTTP客户端和HTTP服务器的功能，较之<code>requests</code>有更好的获取数据的性能，有兴趣可以阅读它的<a target="_blank" rel="noopener" href="https://aiohttp.readthedocs.io/en/stable/">官方文档</a>。</p>
<pre><code class="Python">import asyncio
import aiohttp


async def download(url):
    print(&#39;Fetch:&#39;, url)
    async with aiohttp.ClientSession() as session:
        async with session.get(url, ssl=False) as resp:
            print(url, &#39;---&gt;&#39;, resp.status)
            print(url, &#39;---&gt;&#39;, resp.headers)
            print(&#39;\n\n&#39;, await resp.text())


def main():
    loop = asyncio.get_event_loop()
    urls = [
        &#39;https://www.baidu.com&#39;,
        &#39;http://www.sohu.com/&#39;,
        &#39;http://www.sina.com.cn/&#39;,
        &#39;https://www.taobao.com/&#39;,
        &#39;http://jd.com/&#39;
    ]
    tasks = [download(url) for url in urls]
    loop.run_until_complete(asyncio.wait(tasks))
    loop.close()


if __name__ == &#39;__main__&#39;:
    main()
</code></pre>
<h3 id="实例-多线程爬取“手机搜狐网”所有页面"><a href="#实例-多线程爬取“手机搜狐网”所有页面" class="headerlink" title="实例 - 多线程爬取“手机搜狐网”所有页面"></a>实例 - 多线程爬取“手机搜狐网”所有页面</h3><p>下面我们把之间讲的所有知识结合起来，用面向对象的方式实现一个爬取“手机搜狐网”的多线程爬虫。</p>
<pre><code class="Python">import pickle
import zlib
from enum import Enum, unique
from hashlib import sha1
from random import random
from threading import Thread, current_thread, local
from time import sleep
from urllib.parse import urlparse

import pymongo
import redis
import requests
from bs4 import BeautifulSoup
from bson import Binary


@unique
class SpiderStatus(Enum):
    IDLE = 0
    WORKING = 1


def decode_page(page_bytes, charsets=(&#39;utf-8&#39;,)):
    page_html = None
    for charset in charsets:
        try:
            page_html = page_bytes.decode(charset)
            break
        except UnicodeDecodeError:
            pass
    return page_html


class Retry(object):

    def __init__(self, *, retry_times=3,
                 wait_secs=5, errors=(Exception, )):
        self.retry_times = retry_times
        self.wait_secs = wait_secs
        self.errors = errors

    def __call__(self, fn):

        def wrapper(*args, **kwargs):
            for _ in range(self.retry_times):
                try:
                    return fn(*args, **kwargs)
                except self.errors as e:
                    print(e)
                    sleep((random() + 1) * self.wait_secs)
            return None

        return wrapper


class Spider(object):

    def __init__(self):
        self.status = SpiderStatus.IDLE

    @Retry()
    def fetch(self, current_url, *, charsets=(&#39;utf-8&#39;, ),
              user_agent=None, proxies=None):
        thread_name = current_thread().name
        print(f&#39;[&#123;thread_name&#125;]: &#123;current_url&#125;&#39;)
        headers = &#123;&#39;user-agent&#39;: user_agent&#125; if user_agent else &#123;&#125;
        resp = requests.get(current_url,
                            headers=headers, proxies=proxies)
        return decode_page(resp.content, charsets) \
            if resp.status_code == 200 else None

    def parse(self, html_page, *, domain=&#39;m.sohu.com&#39;):
        soup = BeautifulSoup(html_page, &#39;lxml&#39;)
        for a_tag in soup.body.select(&#39;a[href]&#39;):
            parser = urlparse(a_tag.attrs[&#39;href&#39;])
            scheme = parser.scheme or &#39;http&#39;
            netloc = parser.netloc or domain
            if scheme != &#39;javascript&#39; and netloc == domain:
                path = parser.path
                query = &#39;?&#39; + parser.query if parser.query else &#39;&#39;
                full_url = f&#39;&#123;scheme&#125;://&#123;netloc&#125;&#123;path&#125;&#123;query&#125;&#39;
                redis_client = thread_local.redis_client
                if not redis_client.sismember(&#39;visited_urls&#39;, full_url):
                    redis_client.rpush(&#39;m_sohu_task&#39;, full_url)

    def extract(self, html_page):
        pass

    def store(self, data_dict):
        # redis_client = thread_local.redis_client
        # mongo_db = thread_local.mongo_db
        pass


class SpiderThread(Thread):

    def __init__(self, name, spider):
        super().__init__(name=name, daemon=True)
        self.spider = spider

    def run(self):
        redis_client = redis.Redis(host=&#39;1.2.3.4&#39;, port=6379, password=&#39;1qaz2wsx&#39;)
        mongo_client = pymongo.MongoClient(host=&#39;1.2.3.4&#39;, port=27017)
        thread_local.redis_client = redis_client
        thread_local.mongo_db = mongo_client.msohu 
        while True:
            current_url = redis_client.lpop(&#39;m_sohu_task&#39;)
            while not current_url:
                current_url = redis_client.lpop(&#39;m_sohu_task&#39;)
            self.spider.status = SpiderStatus.WORKING
            current_url = current_url.decode(&#39;utf-8&#39;)
            if not redis_client.sismember(&#39;visited_urls&#39;, current_url):
                redis_client.sadd(&#39;visited_urls&#39;, current_url)
                html_page = self.spider.fetch(current_url)
                if html_page not in [None, &#39;&#39;]:
                    hasher = hasher_proto.copy()
                    hasher.update(current_url.encode(&#39;utf-8&#39;))
                    doc_id = hasher.hexdigest()
                    sohu_data_coll = mongo_client.msohu.webpages
                    if not sohu_data_coll.find_one(&#123;&#39;_id&#39;: doc_id&#125;):
                        sohu_data_coll.insert_one(&#123;
                            &#39;_id&#39;: doc_id,
                            &#39;url&#39;: current_url,
                            &#39;page&#39;: Binary(zlib.compress(pickle.dumps(html_page)))
                        &#125;)
                    self.spider.parse(html_page)
            self.spider.status = SpiderStatus.IDLE


def is_any_alive(spider_threads):
    return any([spider_thread.spider.status == SpiderStatus.WORKING
                for spider_thread in spider_threads])


thread_local = local()
hasher_proto = sha1()


def main():
    redis_client = redis.Redis(host=&#39;1.2.3.4&#39;, port=6379, password=&#39;1qaz2wsx&#39;)
    if not redis_client.exists(&#39;m_sohu_task&#39;):
        redis_client.rpush(&#39;m_sohu_task&#39;, &#39;http://m.sohu.com/&#39;)

    spider_threads = [SpiderThread(&#39;thread-%d&#39; % i, Spider())
                      for i in range(10)]
    for spider_thread in spider_threads:
        spider_thread.start()

    while redis_client.exists(&#39;m_sohu_task&#39;) or is_any_alive(spider_threads):
        sleep(5)

    print(&#39;Over!&#39;)


if __name__ == &#39;__main__&#39;:
    main()
</code></pre>

        
        
          <blockquote id="copyright">
              <p>Original link: <a href="http://example.com/page/6/index.html">http://example.com/page/6/index.html</a></p>
              <p>Copyright Notice: 转载请注明出处.</p>
          </blockquote>
        
      
    </div>
    <footer class="article-footer">
      
        <div class="article-tag-wrap">
          

          
          
    <div class="social-share">
      <span>Share:</span>
    </div>



        </div>
      
      
      
    </footer>
  </div>
</article>


  <article id="post-Python-100-Days-master/Day61-65/63.存储数据" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-inner">
    
    
    <div class="article-meta">
      
      <span class="article-date">
  <i class="fa fa-date"></i>
  <time class="dt-published" datetime="2021-10-17T13:24:55.575Z" itemprop="datePublished">2021年10月17日</time>
</span>
      
      
      
<a href="/2021/10/17/Python-100-Days-master/Day61-65/63.%E5%AD%98%E5%82%A8%E6%95%B0%E6%8D%AE/#comments" class="article-comment-link">
  
  <i class="fa fa-commt"></i>
  Guestbook
</a>


    </div>
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="存储数据"><a href="#存储数据" class="headerlink" title="存储数据"></a>存储数据</h2><h3 id="存储海量数据"><a href="#存储海量数据" class="headerlink" title="存储海量数据"></a>存储海量数据</h3><p>数据持久化的首选方案应该是关系型数据库，关系型数据库的产品很多，包括：Oracle、MySQL、SQLServer、PostgreSQL等。如果要存储海量的低价值数据，文档数据库也是不错的选择，MongoDB是文档数据库中的佼佼者，有兴趣的读者可以自行研究。</p>
<p>下面的代码演示了如何使用MySQL来保存从知乎发现上爬取到的链接和页面。</p>
<pre><code class="SQL">create database zhihu default charset utf8;
create user &#39;hellokitty&#39;@&#39;%&#39; identified by &#39;Hellokitty.618&#39;;
grant all privileges on zhihu.* to &#39;hellokitty&#39;@&#39;%&#39;;
flush privileges;

use zhihu;
create table `tb_explore`
(
    `id` integer auto_increment,
    `url` varchar(1024) not null,
    `page` longblob not null,
    `digest` char(48) unique not null,
    `idate` datetime default now(),
    primary key (`id`)
);
</code></pre>
<pre><code class="Python">import hashlib
import pickle
import re
import zlib
from urllib.parse import urljoin

import MySQLdb
import bs4
import requests

conn = MySQLdb.connect(host=&#39;1.2.3.4&#39;, port=3306,
                       user=&#39;hellokitty&#39;, password=&#39;Hellokitty.618&#39;,
                       database=&#39;zhihu&#39;, charset=&#39;utf8&#39;,
                       autocommit=True)


def write_to_db(url, page, digest):
    try:
        with conn.cursor() as cursor:
            cursor.execute(
                &#39;insert into tb_explore (url, page, digest) values (%s, %s, %s) &#39;,
                (url, page, digest)
            )
    except MySQLdb.MySQLError as err:
        print(err)


def main():
    base_url = &#39;https://www.zhihu.com/&#39;
    seed_url = urljoin(base_url, &#39;explore&#39;)
    headers = &#123;&#39;user-agent&#39;: &#39;Baiduspider&#39;&#125;
    try:
        resp = requests.get(seed_url, headers=headers)
        soup = bs4.BeautifulSoup(resp.text, &#39;lxml&#39;)
        href_regex = re.compile(r&#39;^/question&#39;)
        for a_tag in soup.find_all(&#39;a&#39;, &#123;&#39;href&#39;: href_regex&#125;):
            href = a_tag.attrs[&#39;href&#39;]
            full_url = urljoin(base_url, href)
            digest = hashlib.sha1(full_url.encode()).hexdigest()
            html_page = requests.get(full_url, headers=headers).text
            zipped_page = zlib.compress(pickle.dumps(html_page))
            write_to_db(full_url, zipped_page, digest)
    finally:
        conn.close()


if __name__ == &#39;__main__&#39;:
    main()
</code></pre>
<h3 id="数据缓存"><a href="#数据缓存" class="headerlink" title="数据缓存"></a>数据缓存</h3><p>通过<a href="./67.%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E5%92%8C%E8%A7%A3%E6%9E%90.md">《网络数据采集和解析》</a>一文，我们已经知道了如何从指定的页面中抓取数据，以及如何保存抓取的结果，但是我们没有考虑过这么一种情况，就是我们可能需要从已经抓取过的页面中提取出更多的数据，重新去下载这些页面对于规模不大的网站倒是问题也不大，但是如果能够把这些页面缓存起来，对应用的性能会有明显的改善。下面的例子演示了如何使用Redis来缓存知乎发现上的页面。</p>
<pre><code class="Python">import hashlib
import pickle
import re
import zlib
from urllib.parse import urljoin

import bs4
import redis
import requests


def main():
    base_url = &#39;https://www.zhihu.com/&#39;
    seed_url = urljoin(base_url, &#39;explore&#39;)
    client = redis.Redis(host=&#39;1.2.3.4&#39;, port=6379, password=&#39;1qaz2wsx&#39;)
    headers = &#123;&#39;user-agent&#39;: &#39;Baiduspider&#39;&#125;
    resp = requests.get(seed_url, headers=headers)
    soup = bs4.BeautifulSoup(resp.text, &#39;lxml&#39;)
    href_regex = re.compile(r&#39;^/question&#39;)
    for a_tag in soup.find_all(&#39;a&#39;, &#123;&#39;href&#39;: href_regex&#125;):
        href = a_tag.attrs[&#39;href&#39;]
        full_url = urljoin(base_url, href)
        field_key = hashlib.sha1(full_url.encode()).hexdigest()
        if not client.hexists(&#39;spider:zhihu:explore&#39;, field_key):
            html_page = requests.get(full_url, headers=headers).text
            zipped_page = zlib.compress(pickle.dumps(html_page))
            client.hset(&#39;spider:zhihu:explore&#39;, field_key, zipped_page)
    print(&#39;Total %d question pages found.&#39; % client.hlen(&#39;spider:zhihu:explore&#39;))


if __name__ == &#39;__main__&#39;:
    main()
</code></pre>

        
        
          <blockquote id="copyright">
              <p>Original link: <a href="http://example.com/page/6/index.html">http://example.com/page/6/index.html</a></p>
              <p>Copyright Notice: 转载请注明出处.</p>
          </blockquote>
        
      
    </div>
    <footer class="article-footer">
      
        <div class="article-tag-wrap">
          

          
          
    <div class="social-share">
      <span>Share:</span>
    </div>



        </div>
      
      
      
    </footer>
  </div>
</article>


  <article id="post-Python-100-Days-master/Day61-65/62.数据采集和解析" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-inner">
    
    
    <div class="article-meta">
      
      <span class="article-date">
  <i class="fa fa-date"></i>
  <time class="dt-published" datetime="2021-10-17T13:24:55.573Z" itemprop="datePublished">2021年10月17日</time>
</span>
      
      
      
<a href="/2021/10/17/Python-100-Days-master/Day61-65/62.%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%E5%92%8C%E8%A7%A3%E6%9E%90/#comments" class="article-comment-link">
  
  <i class="fa fa-commt"></i>
  Guestbook
</a>


    </div>
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="数据采集和解析"><a href="#数据采集和解析" class="headerlink" title="数据采集和解析"></a>数据采集和解析</h2><p>通过上一个章节的讲解，我们已经了解到了开发一个爬虫需要做的工作以及一些常见的问题，下面我们给出一个爬虫开发相关技术的清单以及这些技术涉及到的标准库和第三方库，稍后我们会一一介绍这些内容。</p>
<ol>
<li>下载数据 - <strong>urllib</strong> / <strong>requests</strong> / <strong>aiohttp</strong> / <strong>httpx</strong>。</li>
<li>解析数据 - <strong>re</strong> / <strong>lxml</strong> / <strong>beautifulsoup4</strong> / <strong>pyquery</strong>。</li>
<li>缓存和持久化 - <strong>mysqlclient</strong> / <strong>sqlalchemy</strong> / <strong>peewee</strong> / <strong>redis</strong> / <strong>pymongo</strong>。</li>
<li>生成数字签名 - <strong>hashlib</strong>。</li>
<li>序列化和压缩 - <strong>pickle</strong> / <strong>json</strong> / <strong>zlib</strong>。</li>
<li>调度器 - <strong>multiprocessing</strong> / <strong>threading</strong> / <strong>concurrent.futures</strong>。</li>
</ol>
<h3 id="HTML页面"><a href="#HTML页面" class="headerlink" title="HTML页面"></a>HTML页面</h3><pre><code class="HTML">&lt;!DOCTYPE html&gt;
&lt;html&gt;
    &lt;head&gt;
        &lt;title&gt;Home&lt;/title&gt;
        &lt;style type=&quot;text/css&quot;&gt;
            /* 此处省略层叠样式表代码 */
        &lt;/style&gt;
    &lt;/head&gt;
    &lt;body&gt;
        &lt;div class=&quot;wrapper&quot;&gt;
            &lt;header&gt;
                &lt;h1&gt;Yoko&#39;s Kitchen&lt;/h1&gt;
                &lt;nav&gt;
                    &lt;ul&gt;
                        &lt;li&gt;&lt;a href=&quot;&quot; class=&quot;current&quot;&gt;Home&lt;/a&gt;&lt;/li&gt;
                        &lt;li&gt;&lt;a href=&quot;&quot;&gt;Classes&lt;/a&gt;&lt;/li&gt;
                        &lt;li&gt;&lt;a href=&quot;&quot;&gt;Catering&lt;/a&gt;&lt;/li&gt;
                        &lt;li&gt;&lt;a href=&quot;&quot;&gt;About&lt;/a&gt;&lt;/li&gt;
                        &lt;li&gt;&lt;a href=&quot;&quot;&gt;Contact&lt;/a&gt;&lt;/li&gt;
                    &lt;/ul&gt;
                &lt;/nav&gt;
            &lt;/header&gt;
            &lt;section class=&quot;courses&quot;&gt;
                &lt;article&gt;
                    &lt;figure&gt;
                        &lt;img src=&quot;images/bok-choi.jpg&quot; alt=&quot;Bok Choi&quot; /&gt;
                        &lt;figcaption&gt;Bok Choi&lt;/figcaption&gt;
                    &lt;/figure&gt;
                    &lt;hgroup&gt;
                        &lt;h2&gt;Japanese Vegetarian&lt;/h2&gt;
                        &lt;h3&gt;Five week course in London&lt;/h3&gt;
                    &lt;/hgroup&gt;
                    &lt;p&gt;A five week introduction to traditional Japanese vegetarian meals, teaching you a selection of rice and noodle dishes.&lt;/p&gt;
                &lt;/article&gt;    
                &lt;article&gt;
                    &lt;figure&gt;
                        &lt;img src=&quot;images/teriyaki.jpg&quot; alt=&quot;Teriyaki sauce&quot; /&gt;
                        &lt;figcaption&gt;Teriyaki Sauce&lt;/figcaption&gt;
                    &lt;/figure&gt;
                    &lt;hgroup&gt;
                        &lt;h2&gt;Sauces Masterclass&lt;/h2&gt;
                        &lt;h3&gt;One day workshop&lt;/h3&gt;
                    &lt;/hgroup&gt;
                    &lt;p&gt;An intensive one-day course looking at how to create the most delicious sauces for use in a range of Japanese cookery.&lt;/p&gt;
                &lt;/article&gt;    
            &lt;/section&gt;
            &lt;aside&gt;
                &lt;section class=&quot;popular-recipes&quot;&gt;
                    &lt;h2&gt;Popular Recipes&lt;/h2&gt;
                    &lt;a href=&quot;&quot;&gt;Yakitori (grilled chicken)&lt;/a&gt;
                    &lt;a href=&quot;&quot;&gt;Tsukune (minced chicken patties)&lt;/a&gt;
                    &lt;a href=&quot;&quot;&gt;Okonomiyaki (savory pancakes)&lt;/a&gt;
                    &lt;a href=&quot;&quot;&gt;Mizutaki (chicken stew)&lt;/a&gt;
                &lt;/section&gt;
                &lt;section class=&quot;contact-details&quot;&gt;
                    &lt;h2&gt;Contact&lt;/h2&gt;
                    &lt;p&gt;Yoko&#39;s Kitchen&lt;br&gt;
                        27 Redchurch Street&lt;br&gt;
                        Shoreditch&lt;br&gt;
                        London E2 7DP&lt;/p&gt;
                &lt;/section&gt;
            &lt;/aside&gt;
            &lt;footer&gt;
                &amp;copy; 2011 Yoko&#39;s Kitchen
            &lt;/footer&gt;
        &lt;/div&gt;
        &lt;script&gt;
            /* 此处省略JavaScript代码 */
        &lt;/script&gt;
    &lt;/body&gt;
&lt;/html&gt;
</code></pre>
<p>如上所示的HTML页面通常由三部分构成，分别是用来承载内容的Tag（标签）、负责渲染页面的CSS（层叠样式表）以及控制交互式行为的JavaScript。通常，我们可以在浏览器的右键菜单中通过“查看网页源代码”的方式获取网页的代码并了解页面的结构；当然，我们也可以通过浏览器提供的开发人员工具来了解更多的信息。</p>
<h4 id="使用requests获取页面"><a href="#使用requests获取页面" class="headerlink" title="使用requests获取页面"></a>使用requests获取页面</h4><p>在上一节课的代码中我们使用了三方库<code>requests</code>来获取页面，下面我们对<code>requests</code>库的用法做进一步说明。</p>
<ol>
<li><p>GET请求和POST请求。</p>
<pre><code class="Python">import requests

resp = requests.get(&#39;http://www.baidu.com/index.html&#39;)
print(resp.status_code)
print(resp.headers)
print(resp.cookies)
print(resp.content.decode(&#39;utf-8&#39;))

resp = requests.post(&#39;http://httpbin.org/post&#39;, data=&#123;&#39;name&#39;: &#39;Hao&#39;, &#39;age&#39;: 40&#125;)
print(resp.text)
data = resp.json()
print(type(data))
</code></pre>
</li>
<li><p>URL参数和请求头。</p>
<pre><code class="Python">resp = requests.get(
    url=&#39;https://movie.douban.com/top250&#39;,
    headers=&#123;
        &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) &#39;
                      &#39;AppleWebKit/537.36 (KHTML, like Gecko) &#39;
                      &#39;Chrome/83.0.4103.97 Safari/537.36&#39;,
        &#39;Accept&#39;: &#39;text/html,application/xhtml+xml,application/xml;&#39;
                  &#39;q=0.9,image/webp,image/apng,*/*;&#39;
                  &#39;q=0.8,application/signed-exchange;v=b3;q=0.9&#39;,
        &#39;Accept-Language&#39;: &#39;zh-CN,zh;q=0.9,en;q=0.8&#39;,
    &#125;
)
print(resp.status_code)
</code></pre>
</li>
<li><p>复杂的POST请求（文件上传）。</p>
<pre><code class="Python">resp = requests.post(
    url=&#39;http://httpbin.org/post&#39;,
    files=&#123;&#39;file&#39;: open(&#39;data.xlsx&#39;, &#39;rb&#39;)&#125;
)
print(resp.text)
</code></pre>
</li>
<li><p>操作Cookie。</p>
<pre><code class="Python">cookies = &#123;&#39;key1&#39;: &#39;value1&#39;, &#39;key2&#39;: &#39;value2&#39;&#125;
resp = requests.get(&#39;http://httpbin.org/cookies&#39;, cookies=cookies)
print(resp.text)

jar = requests.cookies.RequestsCookieJar()
jar.set(&#39;tasty_cookie&#39;, &#39;yum&#39;, domain=&#39;httpbin.org&#39;, path=&#39;/cookies&#39;)
jar.set(&#39;gross_cookie&#39;, &#39;blech&#39;, domain=&#39;httpbin.org&#39;, path=&#39;/elsewhere&#39;)
resp = requests.get(&#39;http://httpbin.org/cookies&#39;, cookies=jar)
print(resp.text)
</code></pre>
</li>
<li><p>设置代理服务器。</p>
<pre><code class="Python">requests.get(&#39;https://www.taobao.com&#39;, proxies=&#123;
    &#39;http&#39;: &#39;http://10.10.1.10:3128&#39;,
    &#39;https&#39;: &#39;http://10.10.1.10:1080&#39;,
&#125;)
</code></pre>
<blockquote>
<p><strong>说明</strong>：关于<code>requests</code>库的相关知识，还是强烈建议大家自行阅读它的<a target="_blank" rel="noopener" href="https://requests.readthedocs.io/zh_CN/latest/">官方文档</a>。</p>
</blockquote>
</li>
<li><p>设置请求超时。</p>
<pre><code class="Python">requests.get(&#39;https://github.com&#39;, timeout=10)
</code></pre>
</li>
</ol>
<h3 id="页面解析"><a href="#页面解析" class="headerlink" title="页面解析"></a>页面解析</h3><h4 id="几种解析方式的比较"><a href="#几种解析方式的比较" class="headerlink" title="几种解析方式的比较"></a>几种解析方式的比较</h4><table>
<thead>
<tr>
<th>解析方式</th>
<th>对应的模块</th>
<th>速度</th>
<th>使用难度</th>
<th>备注</th>
</tr>
</thead>
<tbody><tr>
<td>正则表达式解析</td>
<td>re</td>
<td>快</td>
<td>困难</td>
<td>常用正则表达式<br/>在线正则表达式测试</td>
</tr>
<tr>
<td>XPath解析</td>
<td>lxml</td>
<td>快</td>
<td>一般</td>
<td>需要安装C语言依赖库<br/>唯一支持XML的解析器</td>
</tr>
<tr>
<td>CSS选择器解析</td>
<td>bs4 / pyquery</td>
<td>不确定</td>
<td>简单</td>
<td></td>
</tr>
</tbody></table>
<blockquote>
<p><strong>说明</strong>：<code>BeautifulSoup</code>可选的解析器包括：Python标准库中的<code>html.parser</code>、<code>lxml</code>的HTML解析器、<code>lxml</code>的XML解析器和<code>html5lib</code>。</p>
</blockquote>
<h4 id="使用正则表达式解析页面"><a href="#使用正则表达式解析页面" class="headerlink" title="使用正则表达式解析页面"></a>使用正则表达式解析页面</h4><p>如果你对正则表达式没有任何的概念，那么推荐先阅读<a target="_blank" rel="noopener" href="https://deerchao.cn/tutorials/regex/regex.htm">《正则表达式30分钟入门教程》</a>，然后再阅读我们之前讲解在Python中如何使用正则表达式一文。</p>
<p>下面的例子演示了如何用正则表达式解析“豆瓣电影Top250”中的中文电影名称。</p>
<pre><code class="Python">import random
import re
import time

import requests

PATTERN = re.compile(r&#39;&lt;a[^&gt;]*?&gt;\s*&lt;span class=&quot;title&quot;&gt;(.*?)&lt;/span&gt;&#39;)

for page in range(10):
    resp = requests.get(
        url=f&#39;https://movie.douban.com/top250?start=&#123;page * 25&#125;&#39;,
        headers=&#123;
            &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) &#39;
                          &#39;AppleWebKit/537.36 (KHTML, like Gecko) &#39;
                          &#39;Chrome/83.0.4103.97 Safari/537.36&#39;,
            &#39;Accept&#39;: &#39;text/html,application/xhtml+xml,application/xml;&#39;
                      &#39;q=0.9,image/webp,image/apng,*/*;&#39;
                      &#39;q=0.8,application/signed-exchange;v=b3;q=0.9&#39;,
            &#39;Accept-Language&#39;: &#39;zh-CN,zh;q=0.9,en;q=0.8&#39;,
        &#125;,
    )
    items = PATTERN.findall(resp.text)
    for item in items:
        print(item)
    time.sleep(random.randint(1, 5))
</code></pre>
<h4 id="XPath解析和lxml"><a href="#XPath解析和lxml" class="headerlink" title="XPath解析和lxml"></a>XPath解析和lxml</h4><p>XPath是在XML文档中查找信息的一种语法，它使用路径表达式来选取XML文档中的节点或者节点集。这里所说的XPath节点包括元素、属性、文本、命名空间、处理指令、注释、根节点等。</p>
<pre><code class="XML">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;bookstore&gt;
    &lt;book&gt;
      &lt;title lang=&quot;eng&quot;&gt;Harry Potter&lt;/title&gt;
      &lt;price&gt;29.99&lt;/price&gt;
    &lt;/book&gt;
    &lt;book&gt;
      &lt;title lang=&quot;zh&quot;&gt;三国演义&lt;/title&gt;
      &lt;price&gt;39.95&lt;/price&gt;
    &lt;/book&gt;
&lt;/bookstore&gt;
</code></pre>
<p>对于上面的XML文件，我们可以用如下所示的XPath语法获取文档中的节点。</p>
<table>
<thead>
<tr>
<th>路径表达式</th>
<th>结果</th>
</tr>
</thead>
<tbody><tr>
<td>bookstore</td>
<td>选取 bookstore 元素的所有子节点。</td>
</tr>
<tr>
<td>/bookstore</td>
<td>选取根元素 bookstore。注释：假如路径起始于正斜杠( / )，则此路径始终代表到某元素的绝对路径！</td>
</tr>
<tr>
<td>bookstore/book</td>
<td>选取属于 bookstore 的子元素的所有 book 元素。</td>
</tr>
<tr>
<td>//book</td>
<td>选取所有 book 子元素，而不管它们在文档中的位置。</td>
</tr>
<tr>
<td>bookstore//book</td>
<td>选择属于 bookstore 元素的后代的所有 book 元素，而不管它们位于 bookstore 之下的什么位置。</td>
</tr>
<tr>
<td>//@lang</td>
<td>选取名为 lang 的所有属性。</td>
</tr>
</tbody></table>
<p>在使用XPath语法时，还可以使用XPath中的谓词。</p>
<table>
<thead>
<tr>
<th>路径表达式</th>
<th>结果</th>
</tr>
</thead>
<tbody><tr>
<td>/bookstore/book[1]</td>
<td>选取属于 bookstore 子元素的第一个 book 元素。</td>
</tr>
<tr>
<td>/bookstore/book[last()]</td>
<td>选取属于 bookstore 子元素的最后一个 book 元素。</td>
</tr>
<tr>
<td>/bookstore/book[last()-1]</td>
<td>选取属于 bookstore 子元素的倒数第二个 book 元素。</td>
</tr>
<tr>
<td>/bookstore/book[position()&lt;3]</td>
<td>选取最前面的两个属于 bookstore 元素的子元素的 book 元素。</td>
</tr>
<tr>
<td>//title[@lang]</td>
<td>选取所有拥有名为 lang 的属性的 title 元素。</td>
</tr>
<tr>
<td>//title[@lang=’eng’]</td>
<td>选取所有 title 元素，且这些元素拥有值为 eng 的 lang 属性。</td>
</tr>
<tr>
<td>/bookstore/book[price&gt;35.00]</td>
<td>选取 bookstore 元素的所有 book 元素，且其中的 price 元素的值须大于 35.00。</td>
</tr>
<tr>
<td>/bookstore/book[price&gt;35.00]/title</td>
<td>选取 bookstore 元素中的 book 元素的所有 title 元素，且其中的 price 元素的值须大于 35.00。</td>
</tr>
</tbody></table>
<p>XPath还支持通配符用法，如下所示。</p>
<table>
<thead>
<tr>
<th>路径表达式</th>
<th>结果</th>
</tr>
</thead>
<tbody><tr>
<td>/bookstore/*</td>
<td>选取 bookstore 元素的所有子元素。</td>
</tr>
<tr>
<td>//*</td>
<td>选取文档中的所有元素。</td>
</tr>
<tr>
<td>//title[@*]</td>
<td>选取所有带有属性的 title 元素。</td>
</tr>
</tbody></table>
<p>如果要选取多个节点，可以使用如下所示的方法。</p>
<table>
<thead>
<tr>
<th>路径表达式</th>
<th>结果</th>
</tr>
</thead>
<tbody><tr>
<td>//book/title | //book/price</td>
<td>选取 book 元素的所有 title 和 price 元素。</td>
</tr>
<tr>
<td>//title | //price</td>
<td>选取文档中的所有 title 和 price 元素。</td>
</tr>
<tr>
<td>/bookstore/book/title | //price</td>
<td>选取属于 bookstore 元素的 book 元素的所有 title 元素，以及文档中所有的 price 元素。</td>
</tr>
</tbody></table>
<blockquote>
<p><strong>说明</strong>：上面的例子来自于菜鸟教程网站上<a target="_blank" rel="noopener" href="https://www.runoob.com/xpath/xpath-tutorial.html">XPath教程</a>，有兴趣的读者可以自行阅读原文。</p>
</blockquote>
<p>当然，如果不理解或者不太熟悉XPath语法，可以在Chrome浏览器中按照如下所示的方法查看元素的XPath语法。</p>
<p><img src="./res/douban-xpath.png"></p>
<p>下面的例子演示了如何用XPath解析“豆瓣电影Top250”中的中文电影名称。</p>
<pre><code class="Python">from lxml import etree

import requests

for page in range(10):
    resp = requests.get(
        url=f&#39;https://movie.douban.com/top250?start=&#123;page * 25&#125;&#39;,
        headers=&#123;
            &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) &#39;
                          &#39;AppleWebKit/537.36 (KHTML, like Gecko) &#39;
                          &#39;Chrome/83.0.4103.97 Safari/537.36&#39;,
            &#39;Accept&#39;: &#39;text/html,application/xhtml+xml,application/xml;&#39;
                      &#39;q=0.9,image/webp,image/apng,*/*;&#39;
                      &#39;q=0.8,application/signed-exchange;v=b3;q=0.9&#39;,
            &#39;Accept-Language&#39;: &#39;zh-CN,zh;q=0.9,en;q=0.8&#39;,
        &#125;
    )
    html = etree.HTML(resp.text)
    spans = html.xpath(&#39;/html/body/div[3]/div[1]/div/div[1]/ol/li/div/div[2]/div[1]/a/span[1]&#39;)
    for span in spans:
        print(span.text)
</code></pre>
<h3 id="BeautifulSoup的使用"><a href="#BeautifulSoup的使用" class="headerlink" title="BeautifulSoup的使用"></a>BeautifulSoup的使用</h3><p>BeautifulSoup是一个可以从HTML或XML文件中提取数据的Python库。它能够通过你喜欢的转换器实现惯用的文档导航、查找、修改文档的方式。</p>
<ol>
<li>遍历文档树<ul>
<li>获取标签</li>
<li>获取标签属性</li>
<li>获取标签内容</li>
<li>获取子（孙）节点</li>
<li>获取父节点/祖先节点</li>
<li>获取兄弟节点</li>
</ul>
</li>
<li>搜索树节点<ul>
<li>find / find_all</li>
<li>select_one / select</li>
</ul>
</li>
</ol>
<blockquote>
<p><strong>说明</strong>：更多内容可以参考BeautifulSoup的<a target="_blank" rel="noopener" href="https://beautifulsoup.readthedocs.io/zh_CN/v4.4.0/">官方文档</a>。</p>
</blockquote>
<p>下面的例子演示了如何用CSS选择器解析“豆瓣电影Top250”中的中文电影名称。</p>
<pre><code class="Python">import random
import time

import bs4
import requests

for page in range(10):
    resp = requests.get(
        url=f&#39;https://movie.douban.com/top250?start=&#123;page * 25&#125;&#39;,
        headers=&#123;
            &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) &#39;
                          &#39;AppleWebKit/537.36 (KHTML, like Gecko) &#39;
                          &#39;Chrome/83.0.4103.97 Safari/537.36&#39;,
            &#39;Accept&#39;: &#39;text/html,application/xhtml+xml,application/xml;&#39;
                      &#39;q=0.9,image/webp,image/apng,*/*;&#39;
                      &#39;q=0.8,application/signed-exchange;v=b3;q=0.9&#39;,
            &#39;Accept-Language&#39;: &#39;zh-CN,zh;q=0.9,en;q=0.8&#39;,
        &#125;,
    )
    soup = bs4.BeautifulSoup(resp.text, &#39;lxml&#39;)
    elements = soup.select(&#39;.info&gt;div&gt;a&#39;)
    for element in elements:
        span = element.select_one(&#39;.title&#39;)
        print(span.text)
    time.sleep(random.random() * 5)
</code></pre>
<h3 id="例子-获取知乎发现上的问题链接"><a href="#例子-获取知乎发现上的问题链接" class="headerlink" title="例子 - 获取知乎发现上的问题链接"></a>例子 - 获取知乎发现上的问题链接</h3><pre><code class="Python">import re
from urllib.parse import urljoin

import bs4
import requests


def main():
    headers = &#123;&#39;user-agent&#39;: &#39;Baiduspider&#39;&#125;
    base_url = &#39;https://www.zhihu.com/&#39;
    resp = requests.get(urljoin(base_url, &#39;explore&#39;), headers=headers)
    soup = bs4.BeautifulSoup(resp.text, &#39;lxml&#39;)
    href_regex = re.compile(r&#39;^/question&#39;)
    links_set = set()
    for a_tag in soup.find_all(&#39;a&#39;, &#123;&#39;href&#39;: href_regex&#125;):
        if &#39;href&#39; in a_tag.attrs:
            href = a_tag.attrs[&#39;href&#39;]
            full_url = urljoin(base_url, href)
            links_set.add(full_url)
    print(&#39;Total %d question pages found.&#39; % len(links_set))
    print(links_set)


if __name__ == &#39;__main__&#39;:
    main()
</code></pre>

        
        
          <blockquote id="copyright">
              <p>Original link: <a href="http://example.com/page/6/index.html">http://example.com/page/6/index.html</a></p>
              <p>Copyright Notice: 转载请注明出处.</p>
          </blockquote>
        
      
    </div>
    <footer class="article-footer">
      
        <div class="article-tag-wrap">
          

          
          
    <div class="social-share">
      <span>Share:</span>
    </div>



        </div>
      
      
      
    </footer>
  </div>
</article>


  <article id="post-Python-100-Days-master/Day61-65/61.网络爬虫和相关工具" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-inner">
    
    
    <div class="article-meta">
      
      <span class="article-date">
  <i class="fa fa-date"></i>
  <time class="dt-published" datetime="2021-10-17T13:24:55.570Z" itemprop="datePublished">2021年10月17日</time>
</span>
      
      
      
<a href="/2021/10/17/Python-100-Days-master/Day61-65/61.%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%E5%92%8C%E7%9B%B8%E5%85%B3%E5%B7%A5%E5%85%B7/#comments" class="article-comment-link">
  
  <i class="fa fa-commt"></i>
  Guestbook
</a>


    </div>
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="网络爬虫和相关工具"><a href="#网络爬虫和相关工具" class="headerlink" title="网络爬虫和相关工具"></a>网络爬虫和相关工具</h2><h3 id="网络爬虫的概念"><a href="#网络爬虫的概念" class="headerlink" title="网络爬虫的概念"></a>网络爬虫的概念</h3><p>网络爬虫（web crawler），以前经常称之为网络蜘蛛（spider），是按照一定的规则自动浏览万维网并获取信息的机器人程序（或脚本），曾经被广泛的应用于互联网搜索引擎。使用过互联网和浏览器的人都知道，网页中除了供用户阅读的文字信息之外，还包含一些超链接。网络爬虫系统正是通过网页中的超链接信息不断获得网络上的其它页面。正因如此，网络数据采集的过程就像一个爬虫或者蜘蛛在网络上漫游，所以才被形象的称为网络爬虫或者网络蜘蛛。</p>
<h4 id="爬虫的应用领域"><a href="#爬虫的应用领域" class="headerlink" title="爬虫的应用领域"></a>爬虫的应用领域</h4><p>在理想的状态下，所有ICP（Internet Content Provider）都应该为自己的网站提供API接口来共享它们允许其他程序获取的数据，在这种情况下爬虫就不是必需品，国内比较有名的电商平台（如淘宝、京东等）、社交平台（如腾讯微博等）等网站都提供了自己的Open API，但是这类Open API通常会对可以抓取的数据以及抓取数据的频率进行限制。对于大多数的公司而言，及时的获取行业相关数据是企业生存的重要环节之一，然而大部分企业在行业数据方面的匮乏是其与生俱来的短板，合理的利用爬虫来获取数据并从中提取出有商业价值的信息是至关重要的。当然爬虫还有很多重要的应用领域，下面列举了其中的一部分：</p>
<ol>
<li>搜索引擎</li>
<li>新闻聚合</li>
<li>社交应用</li>
<li>舆情监控</li>
<li>行业数据</li>
</ol>
<h3 id="合法性和背景调研"><a href="#合法性和背景调研" class="headerlink" title="合法性和背景调研"></a>合法性和背景调研</h3><h4 id="爬虫合法性探讨"><a href="#爬虫合法性探讨" class="headerlink" title="爬虫合法性探讨"></a>爬虫合法性探讨</h4><ol>
<li>网络爬虫领域目前还属于拓荒阶段，虽然互联网世界已经通过自己的游戏规则建立起一定的道德规范(Robots协议，全称是“网络爬虫排除标准”)，但法律部分还在建立和完善中，也就是说，现在这个领域暂时还是灰色地带。</li>
<li>“法不禁止即为许可”，如果爬虫就像浏览器一样获取的是前端显示的数据（网页上的公开信息）而不是网站后台的私密敏感信息，就不太担心法律法规的约束，因为目前大数据产业链的发展速度远远超过了法律的完善程度。</li>
<li>在爬取网站的时候，需要限制自己的爬虫遵守Robots协议，同时控制网络爬虫程序的抓取数据的速度；在使用数据的时候，必须要尊重网站的知识产权（从Web 2.0时代开始，虽然Web上的数据很多都是由用户提供的，但是网站平台是投入了运营成本的，当用户在注册和发布内容时，平台通常就已经获得了对数据的所有权、使用权和分发权）。如果违反了这些规定，在打官司的时候败诉几率相当高。</li>
</ol>
<h4 id="Robots-txt文件"><a href="#Robots-txt文件" class="headerlink" title="Robots.txt文件"></a>Robots.txt文件</h4><p>大多数网站都会定义robots.txt文件，下面以淘宝的<a target="_blank" rel="noopener" href="http://www.taobao.com/robots.txt">robots.txt</a>文件为例，看看该网站对爬虫有哪些限制。</p>
<pre><code>User-agent:  Baiduspider
Allow:  /article
Allow:  /oshtml
Disallow:  /product/
Disallow:  /

User-Agent:  Googlebot
Allow:  /article
Allow:  /oshtml
Allow:  /product
Allow:  /spu
Allow:  /dianpu
Allow:  /oversea
Allow:  /list
Disallow:  /

User-agent:  Bingbot
Allow:  /article
Allow:  /oshtml
Allow:  /product
Allow:  /spu
Allow:  /dianpu
Allow:  /oversea
Allow:  /list
Disallow:  /

User-Agent:  360Spider
Allow:  /article
Allow:  /oshtml
Disallow:  /

User-Agent:  Yisouspider
Allow:  /article
Allow:  /oshtml
Disallow:  /

User-Agent:  Sogouspider
Allow:  /article
Allow:  /oshtml
Allow:  /product
Disallow:  /

User-Agent:  Yahoo!  Slurp
Allow:  /product
Allow:  /spu
Allow:  /dianpu
Allow:  /oversea
Allow:  /list
Disallow:  /

User-Agent:  *
Disallow:  /
</code></pre>
<p>注意上面robots.txt第一段的最后一行，通过设置“Disallow: /”禁止百度爬虫访问除了“Allow”规定页面外的其他所有页面。因此当你在百度搜索“淘宝”的时候，搜索结果下方会出现：“由于该网站的robots.txt文件存在限制指令（限制搜索引擎抓取），系统无法提供该页面的内容描述”。百度作为一个搜索引擎，至少在表面上遵守了淘宝网的robots.txt协议，所以用户不能从百度上搜索到淘宝内部的产品信息。</p>
<p><img src="./res/baidu-search-taobao.png"> </p>
<h3 id="相关工具介绍"><a href="#相关工具介绍" class="headerlink" title="相关工具介绍"></a>相关工具介绍</h3><h4 id="HTTP协议"><a href="#HTTP协议" class="headerlink" title="HTTP协议"></a>HTTP协议</h4><p>在开始讲解爬虫之前，我们稍微对HTTP（超文本传输协议）做一些回顾，因为我们在网页上看到的内容通常是浏览器执行HTML语言得到的结果，而HTTP就是传输HTML数据的协议。HTTP和其他很多应用级协议一样是构建在TCP（传输控制协议）之上的，它利用了TCP提供的可靠的传输服务实现了Web应用中的数据交换。按照维基百科上的介绍，设计HTTP最初的目的是为了提供一种发布和接收<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/HTML">HTML</a>页面的方法，也就是说这个协议是浏览器和Web服务器之间传输的数据的载体。关于这个协议的详细信息以及目前的发展状况，大家可以阅读阮一峰老师的<a target="_blank" rel="noopener" href="http://www.ruanyifeng.com/blog/2016/08/http.html">《HTTP 协议入门》</a>、<a target="_blank" rel="noopener" href="http://www.ruanyifeng.com/blog/2012/05/internet_protocol_suite_part_i.html">《互联网协议入门》</a>系列以及<a target="_blank" rel="noopener" href="http://www.ruanyifeng.com/blog/2014/09/illustration-ssl.html">《图解HTTPS协议》</a>进行了解，下图是我在四川省网络通信技术重点实验室工作期间用开源协议分析工具Ethereal（抓包工具WireShark的前身）截取的访问百度首页时的HTTP请求和响应的报文（协议数据），由于Ethereal截取的是经过网络适配器的数据，因此可以清晰的看到从物理链路层到应用层的协议数据。</p>
<p>HTTP请求（请求行+请求头+空行+[消息体]）：</p>
<p><img src="./res/http-request.png"></p>
<p>HTTP响应（响应行+响应头+空行+消息体）：</p>
<p><img src="./res/http-response.png"></p>
<blockquote>
<p>说明：但愿这两张如同泛黄照片般的截图帮助你大概的了解到HTTP是一个怎样的协议。 </p>
</blockquote>
<h4 id="相关工具"><a href="#相关工具" class="headerlink" title="相关工具"></a>相关工具</h4><ol>
<li><p>Chrome Developer Tools：谷歌浏览器内置的开发者工具。</p>
<p><img src="./res/chrome-developer-tools.png"></p>
</li>
<li><p>Postman：功能强大的网页调试与RESTful请求工具。</p>
<p><img src="./res/postman.png"></p>
</li>
<li><p>HTTPie：命令行HTTP客户端。</p>
<pre><code class="Bash">pip3 install httpie
</code></pre>
<pre><code class="Bash">http --header http://www.scu.edu.cn
HTTP/1.1 200 OK
Accept-Ranges: bytes
Cache-Control: private, max-age=600
Connection: Keep-Alive
Content-Encoding: gzip
Content-Language: zh-CN
Content-Length: 14403
Content-Type: text/html
Date: Sun, 27 May 2018 15:38:25 GMT
ETag: &quot;e6ec-56d3032d70a32-gzip&quot;
Expires: Sun, 27 May 2018 15:48:25 GMT
Keep-Alive: timeout=5, max=100
Last-Modified: Sun, 27 May 2018 13:44:22 GMT
Server: VWebServer
Vary: User-Agent,Accept-Encoding
X-Frame-Options: SAMEORIGIN
</code></pre>
</li>
<li><p><code>builtwith</code>库：识别网站所用技术的工具。</p>
<pre><code class="Bash">pip3 install builtwith
</code></pre>
<pre><code class="Python">&gt;&gt;&gt; import builtwith
&gt;&gt;&gt; builtwith.parse(&#39;http://www.bootcss.com/&#39;)
&#123;&#39;web-servers&#39;: [&#39;Nginx&#39;], &#39;font-scripts&#39;: [&#39;Font Awesome&#39;], &#39;javascript-frameworks&#39;: [&#39;Lo-dash&#39;, &#39;Underscore.js&#39;, &#39;Vue.js&#39;, &#39;Zepto&#39;, &#39;jQuery&#39;], &#39;web-frameworks&#39;: [&#39;Twitter Bootstrap&#39;]&#125;
&gt;&gt;&gt;
&gt;&gt;&gt; import ssl
&gt;&gt;&gt; ssl._create_default_https_context = ssl._create_unverified_context
&gt;&gt;&gt; builtwith.parse(&#39;https://www.jianshu.com/&#39;)
&#123;&#39;web-servers&#39;: [&#39;Tengine&#39;], &#39;web-frameworks&#39;: [&#39;Twitter Bootstrap&#39;, &#39;Ruby on Rails&#39;], &#39;programming-languages&#39;: [&#39;Ruby&#39;]&#125;
</code></pre>
</li>
<li><p><code>python-whois</code>库：查询网站所有者的工具。</p>
<pre><code class="Bash">pip3 install python-whois
</code></pre>
<pre><code class="Python">&gt;&gt;&gt; import whois
&gt;&gt;&gt; whois.whois(&#39;baidu.com&#39;)
&#123;&#39;domain_name&#39;: [&#39;BAIDU.COM&#39;, &#39;baidu.com&#39;], &#39;registrar&#39;: &#39;MarkMonitor, Inc.&#39;, &#39;whois_server&#39;: &#39;whois.markmonitor.com&#39;, &#39;referral_url&#39;: None, &#39;updated_date&#39;: [datetime.datetime(2017, 7, 28, 2, 36, 28), datetime.datetime(2017, 7, 27, 19, 36, 28)], &#39;creation_date&#39;: [datetime.datetime(1999, 10, 11, 11, 5, 17), datetime.datetime(1999, 10, 11, 4, 5, 17)], &#39;expiration_date&#39;: [datetime.datetime(2026, 10, 11, 11, 5, 17), datetime.datetime(2026, 10, 11, 0, 0)], &#39;name_servers&#39;: [&#39;DNS.BAIDU.COM&#39;, &#39;NS2.BAIDU.COM&#39;, &#39;NS3.BAIDU.COM&#39;, &#39;NS4.BAIDU.COM&#39;, &#39;NS7.BAIDU.COM&#39;, &#39;dns.baidu.com&#39;, &#39;ns4.baidu.com&#39;, &#39;ns3.baidu.com&#39;, &#39;ns7.baidu.com&#39;, &#39;ns2.baidu.com&#39;], &#39;status&#39;: [&#39;clientDeleteProhibited https://icann.org/epp#clientDeleteProhibited&#39;, &#39;clientTransferProhibited https://icann.org/epp#clientTransferProhibited&#39;, &#39;clientUpdateProhibited https://icann.org/epp#clientUpdateProhibited&#39;, &#39;serverDeleteProhibited https://icann.org/epp#serverDeleteProhibited&#39;, &#39;serverTransferProhibited https://icann.org/epp#serverTransferProhibited&#39;, &#39;serverUpdateProhibited https://icann.org/epp#serverUpdateProhibited&#39;, &#39;clientUpdateProhibited (https://www.icann.org/epp#clientUpdateProhibited)&#39;, &#39;clientTransferProhibited (https://www.icann.org/epp#clientTransferProhibited)&#39;, &#39;clientDeleteProhibited (https://www.icann.org/epp#clientDeleteProhibited)&#39;, &#39;serverUpdateProhibited (https://www.icann.org/epp#serverUpdateProhibited)&#39;, &#39;serverTransferProhibited (https://www.icann.org/epp#serverTransferProhibited)&#39;, &#39;serverDeleteProhibited (https://www.icann.org/epp#serverDeleteProhibited)&#39;], &#39;emails&#39;: [&#39;abusecomplaints@markmonitor.com&#39;, &#39;whoisrelay@markmonitor.com&#39;], &#39;dnssec&#39;: &#39;unsigned&#39;, &#39;name&#39;: None, &#39;org&#39;: &#39;Beijing Baidu Netcom Science Technology Co., Ltd.&#39;, &#39;address&#39;: None, &#39;city&#39;: None, &#39;state&#39;: &#39;Beijing&#39;, &#39;zipcode&#39;: None, &#39;country&#39;: &#39;CN&#39;&#125;
</code></pre>
</li>
<li><p><code>robotparser</code>模块：解析<code>robots.txt</code>的工具。</p>
<pre><code class="Python">&gt;&gt;&gt; from urllib import robotparser
&gt;&gt;&gt; parser = robotparser.RobotFileParser()
&gt;&gt;&gt; parser.set_url(&#39;https://www.taobao.com/robots.txt&#39;)
&gt;&gt;&gt; parser.read()
&gt;&gt;&gt; parser.can_fetch(&#39;Baiduspider&#39;, &#39;http://www.taobao.com/article&#39;)
True
&gt;&gt;&gt; parser.can_fetch(&#39;Baiduspider&#39;, &#39;http://www.taobao.com/product&#39;)
False
</code></pre>
</li>
</ol>
<h3 id="一个简单的爬虫"><a href="#一个简单的爬虫" class="headerlink" title="一个简单的爬虫"></a>一个简单的爬虫</h3><p>一个基本的爬虫通常分为数据采集（网页下载）、数据处理（网页解析）和数据存储（将有用的信息持久化）三个部分的内容，当然更为高级的爬虫在数据采集和处理时会使用并发编程或分布式技术，这就需要有调度器（安排线程或进程执行对应的任务）、后台管理程序（监控爬虫的工作状态以及检查数据抓取的结果）等的参与。</p>
<p><img src="./res/crawler-workflow.png"></p>
<p>一般来说，爬虫的工作流程包括以下几个步骤：</p>
<ol>
<li>设定抓取目标（种子页面/起始页面）并获取网页。</li>
<li>当服务器无法访问时，按照指定的重试次数尝试重新下载页面。</li>
<li>在需要的时候设置用户代理或隐藏真实IP，否则可能无法访问页面。</li>
<li>对获取的页面进行必要的解码操作然后抓取出需要的信息。</li>
<li>在获取的页面中通过某种方式（如正则表达式）抽取出页面中的链接信息。</li>
<li>对链接进行进一步的处理（获取页面并重复上面的动作）。</li>
<li>将有用的信息进行持久化以备后续的处理。</li>
</ol>
<p>下面的例子给出了一个从“搜狐体育”上获取NBA新闻标题和链接的爬虫。</p>
<pre><code class="Python">import re
from collections import deque
from urllib.parse import urljoin

import requests

LI_A_PATTERN = re.compile(r&#39;&lt;li class=&quot;item&quot;&gt;.*?&lt;/li&gt;&#39;)
A_TEXT_PATTERN = re.compile(r&#39;&lt;a\s+[^&gt;]*?&gt;(.*?)&lt;/a&gt;&#39;)
A_HREF_PATTERN = re.compile(r&#39;&lt;a\s+[^&gt;]*?href=&quot;(.*?)&quot;\s*[^&gt;]*?&gt;&#39;)


def decode_page(page_bytes, charsets):
    &quot;&quot;&quot;通过指定的字符集对页面进行解码&quot;&quot;&quot;
    for charset in charsets:
        try:
            return page_bytes.decode(charset)
        except UnicodeDecodeError:
            pass


def get_matched_parts(content_string, pattern):
    &quot;&quot;&quot;从字符串中提取所有跟正则表达式匹配的内容&quot;&quot;&quot;
    return pattern.findall(content_string, re.I) \
        if content_string else []


def get_matched_part(content_string, pattern, group_no=1):
    &quot;&quot;&quot;从字符串中提取跟正则表达式匹配的内容&quot;&quot;&quot;
    match = pattern.search(content_string)
    if match:
        return match.group(group_no)


def get_page_html(seed_url, *, charsets=(&#39;utf-8&#39;, )):
    &quot;&quot;&quot;获取页面的HTML代码&quot;&quot;&quot;
    resp = requests.get(seed_url)
    if resp.status_code == 200:
        return decode_page(resp.content, charsets)


def repair_incorrect_href(current_url, href):
    &quot;&quot;&quot;修正获取的href属性&quot;&quot;&quot;
    if href.startswith(&#39;//&#39;):
        href = urljoin(&#39;http://&#39;, href)
    elif href.startswith(&#39;/&#39;):
        href = urljoin(current_url, href)
    return href if href.startswith(&#39;http&#39;) else &#39;&#39;


def start_crawl(seed_url, pattern, *, max_depth=-1):
    &quot;&quot;&quot;开始爬取数据&quot;&quot;&quot;
    new_urls, visited_urls = deque(), set()
    new_urls.append((seed_url, 0))
    while new_urls:
        current_url, depth = new_urls.popleft()
        if depth != max_depth:
            page_html = get_page_html(current_url, charsets=(&#39;utf-8&#39;, &#39;gbk&#39;))
            contents = get_matched_parts(page_html, pattern)
            for content in contents:
                text = get_matched_part(content, A_TEXT_PATTERN)
                href = get_matched_part(content, A_HREF_PATTERN)
                if href:
                    href = repair_incorrect_href(href)
                print(text, href)
                if href and href not in visited_urls:
                    new_urls.append((href, depth + 1))


def main():
    &quot;&quot;&quot;主函数&quot;&quot;&quot;
    start_crawl(
        seed_url=&#39;http://sports.sohu.com/nba_a.shtml&#39;,
        pattern=LI_A_PATTERN,
        max_depth=2
    )


if __name__ == &#39;__main__&#39;:
    main()
</code></pre>
<h3 id="爬虫注意事项"><a href="#爬虫注意事项" class="headerlink" title="爬虫注意事项"></a>爬虫注意事项</h3><p>通过上面的例子，我们对爬虫已经有了一个感性的认识，在编写爬虫时有以下一些注意事项：</p>
<ol>
<li><p>上面的代码使用了<code>requests</code>三方库来获取网络资源，这是一个非常优质的三方库，关于它的用法可以参考它的<a target="_blank" rel="noopener" href="https://requests.readthedocs.io/zh_CN/latest/">官方文档</a>。</p>
</li>
<li><p>上面的代码中使用了双端队列（<code>deque</code>）来保存待爬取的URL。双端队列相当于是使用链式存储结构的<code>list</code>，在双端队列的头尾添加和删除元素性能都比较好，刚好可以用来构造一个FIFO（先进先出）的队列结构。</p>
</li>
<li><p>处理相对路径。有的时候我们从页面中获取的链接不是一个完整的绝对链接而是一个相对链接，这种情况下需要将其与URL前缀进行拼接（<code>urllib.parse</code>中的<code>urljoin()</code>函数可以完成此项操作）。</p>
</li>
<li><p>设置代理服务。有些网站会限制访问的区域（例如美国的Netflix屏蔽了很多国家的访问），有些爬虫需要隐藏自己的身份，在这种情况下可以设置使用代理服务器，代理服务器有免费的服务器和付费的商业服务器，但后者稳定性和可用性都更好，强烈建议在商业项目中使用付费的商业代理服务器。如果使用<code>requests</code>三方库，可以在请求方法中添加<code>proxies</code>参数来指定代理服务器；如果使用标准库，可以通过修改<code>urllib.request</code>中的<code>ProxyHandler</code>来为请求设置代理服务器。</p>
</li>
<li><p>限制下载速度。如果我们的爬虫获取网页的速度过快，可能就会面临被封禁或者产生“损害动产”的风险（这个可能会导致吃官司且败诉），可以在两次获取页面数据之间添加延时从而对爬虫进行限速。</p>
</li>
<li><p>避免爬虫陷阱。有些网站会动态生成页面内容，这会导致产生无限多的页面（例如在线万年历通常会有无穷无尽的链接）。可以通过记录到达当前页面经过了多少个链接（链接深度）来解决该问题，当达到事先设定的最大深度时，爬虫就不再像队列中添加该网页中的链接了。</p>
</li>
<li><p>避开蜜罐链接。网站上的有些链接是浏览器中不可见的，这种链接通常是故意诱使爬虫去访问的蜜罐，一旦访问了这些链接，服务器就会判定请求是来自于爬虫的，这样可能会导致被服务器封禁IP地址。如何避开这些蜜罐链接我们在后面为大家进行讲解。</p>
</li>
<li><p>SSL相关问题。如果使用标准库的<code>urlopen</code>打开一个HTTPS链接时会验证一次SSL证书，如果不做出处理会产生错误提示“SSL: CERTIFICATE_VERIFY_FAILED”，可以通过以下两种方式加以解决：</p>
<ul>
<li><p>使用未经验证的上下文</p>
<pre><code class="Python">import ssl

request = urllib.request.Request(url=&#39;...&#39;, headers=&#123;...&#125;) 
context = ssl._create_unverified_context()
web_page = urllib.request.urlopen(request, context=context)
</code></pre>
</li>
<li><p>设置全局性取消证书验证</p>
<pre><code class="Python">import ssl

ssl._create_default_https_context = ssl._create_unverified_context
</code></pre>
</li>
</ul>
</li>
</ol>

        
        
          <blockquote id="copyright">
              <p>Original link: <a href="http://example.com/page/6/index.html">http://example.com/page/6/index.html</a></p>
              <p>Copyright Notice: 转载请注明出处.</p>
          </blockquote>
        
      
    </div>
    <footer class="article-footer">
      
        <div class="article-tag-wrap">
          

          
          
    <div class="social-share">
      <span>Share:</span>
    </div>



        </div>
      
      
      
    </footer>
  </div>
</article>


  <article id="post-Python-100-Days-master/Day56-60/56-60.用FastAPI开发数据接口" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-inner">
    
    
    <div class="article-meta">
      
      <span class="article-date">
  <i class="fa fa-date"></i>
  <time class="dt-published" datetime="2021-10-17T13:24:55.541Z" itemprop="datePublished">2021年10月17日</time>
</span>
      
      
      
<a href="/2021/10/17/Python-100-Days-master/Day56-60/56-60.%E7%94%A8FastAPI%E5%BC%80%E5%8F%91%E6%95%B0%E6%8D%AE%E6%8E%A5%E5%8F%A3/#comments" class="article-comment-link">
  
  <i class="fa fa-commt"></i>
  Guestbook
</a>


    </div>
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="用FastAPI开发网络数据接口"><a href="#用FastAPI开发网络数据接口" class="headerlink" title="用FastAPI开发网络数据接口"></a>用FastAPI开发网络数据接口</h2><p>FastAPI 是一个用于构建API（网络数据接口）的现代、高性能的Web框架，基于Python 3.6+，使用了Python中的类型提示进行类型检查，非常符合工程化开发的需求，在业界有非常好的口碑。下面，我们先用代码告诉大家FastAPI到底能做什么，然后再来讲解它的方方面面。</p>
<h3 id="FastAPI五分钟上手"><a href="#FastAPI五分钟上手" class="headerlink" title="FastAPI五分钟上手"></a>FastAPI五分钟上手</h3><ol>
<li><p>安装依赖库和ASGI服务器（支持异步I/O的Python服务器）。</p>
<pre><code class="Bash">pip install fastapi
pip install uvicorn
</code></pre>
</li>
<li><p>编写代码<code>main.py</code>。</p>
<pre><code class="Python">from fastapi import FastAPI

app = FastAPI()


@app.get(&#39;/&#39;)
def say_hello():
    return &#123;&#39;code&#39;: 200, &#39;message&#39;: &#39;hello, world!&#39;&#125;
</code></pre>
</li>
<li><p>运行服务。</p>
<pre><code class="Bash">uvicorn main:app --reload
</code></pre>
<blockquote>
<p><strong>说明</strong>：上面运行uvicorn时使用的<code>--reload</code>参数会在代码发生变更时自动重新加载新的内容，这个参数在开发阶段非常的有用。</p>
</blockquote>
</li>
<li><p>访问服务。</p>
</li>
</ol>
<p>  <img src="res/run-first-demo.png"></p>
<ol start="5">
<li><p>查看文档。</p>
<p> <img src="res/first-demo-docs.png"></p>
<blockquote>
<p><strong>注意</strong>：FastAPI会基于<a target="_blank" rel="noopener" href="https://swagger.io/tools/swagger-ui/">Swagger UI</a>自动为数据接口生成对应的文档。</p>
</blockquote>
</li>
</ol>
<h3 id="请求和响应"><a href="#请求和响应" class="headerlink" title="请求和响应"></a>请求和响应</h3><h3 id="接入关系型数据库"><a href="#接入关系型数据库" class="headerlink" title="接入关系型数据库"></a>接入关系型数据库</h3><p>我们可以使用SQLAlchemy三方库来实现对关系型数据库的接入。SQLAlchemy是一个ORM（对象关系映射）框架，ORM框架可以解决Python程序的面向对象模型和关系型数据库的关系模型并不匹配的问题，使得我们可以用面向对象的方式实现数据的CRUD操作。</p>
<h3 id="依赖注入"><a href="#依赖注入" class="headerlink" title="依赖注入"></a>依赖注入</h3><h3 id="中间件"><a href="#中间件" class="headerlink" title="中间件"></a>中间件</h3><h3 id="异步化"><a href="#异步化" class="headerlink" title="异步化"></a>异步化</h3><h3 id="虚拟化部署（Docker）"><a href="#虚拟化部署（Docker）" class="headerlink" title="虚拟化部署（Docker）"></a>虚拟化部署（Docker）</h3><h3 id="项目实战：车辆违章查询"><a href="#项目实战：车辆违章查询" class="headerlink" title="项目实战：车辆违章查询"></a>项目实战：车辆违章查询</h3>
        
        
          <blockquote id="copyright">
              <p>Original link: <a href="http://example.com/page/6/index.html">http://example.com/page/6/index.html</a></p>
              <p>Copyright Notice: 转载请注明出处.</p>
          </blockquote>
        
      
    </div>
    <footer class="article-footer">
      
        <div class="article-tag-wrap">
          

          
          
    <div class="social-share">
      <span>Share:</span>
    </div>



        </div>
      
      
      
    </footer>
  </div>
</article>


  <article id="post-Python-100-Days-master/Day41-55/code/hellodjango/templates/index" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-inner">
    
    
    <div class="article-meta">
      
      <span class="article-date">
  <i class="fa fa-date"></i>
  <time class="dt-published" datetime="2021-10-17T13:24:55.387Z" itemprop="datePublished">2021年10月17日</time>
</span>
      
      
      
<a href="/2021/10/17/Python-100-Days-master/Day41-55/code/hellodjango/templates/index/#comments" class="article-comment-link">
  
  <i class="fa fa-commt"></i>
  Guestbook
</a>


    </div>
    <div class="e-content article-entry" itemprop="articleBody">
      
        <!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <title>首页</title>
        <style>
            #fruits {
                font-size: 1.25em;
            }
        </style>
    </head>
    <body>
        <h1>今天推荐的水果是：</h1>
        <hr>
        <ul id="fruits">
            
        </ul>
    </body>
</html>
        
        
          <blockquote id="copyright">
              <p>Original link: <a href="http://example.com/page/6/index.html">http://example.com/page/6/index.html</a></p>
              <p>Copyright Notice: 转载请注明出处.</p>
          </blockquote>
        
      
    </div>
    <footer class="article-footer">
      
        <div class="article-tag-wrap">
          

          
          
    <div class="social-share">
      <span>Share:</span>
    </div>



        </div>
      
      
      
    </footer>
  </div>
</article>


  <article id="post-Python-100-Days-master/Day41-55/55.项目上线" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-inner">
    
    
    <div class="article-meta">
      
      <span class="article-date">
  <i class="fa fa-date"></i>
  <time class="dt-published" datetime="2021-10-17T13:24:55.329Z" itemprop="datePublished">2021年10月17日</time>
</span>
      
      
      
<a href="/2021/10/17/Python-100-Days-master/Day41-55/55.%E9%A1%B9%E7%9B%AE%E4%B8%8A%E7%BA%BF/#comments" class="article-comment-link">
  
  <i class="fa fa-commt"></i>
  Guestbook
</a>


    </div>
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="项目上线"><a href="#项目上线" class="headerlink" title="项目上线"></a>项目上线</h2><p>请各位读者移步到<a href="../Day91-100/98.%E9%A1%B9%E7%9B%AE%E9%83%A8%E7%BD%B2%E4%B8%8A%E7%BA%BF%E5%92%8C%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98.md">《项目部署上线和性能调优》</a>一文。</p>

        
        
          <blockquote id="copyright">
              <p>Original link: <a href="http://example.com/page/6/index.html">http://example.com/page/6/index.html</a></p>
              <p>Copyright Notice: 转载请注明出处.</p>
          </blockquote>
        
      
    </div>
    <footer class="article-footer">
      
        <div class="article-tag-wrap">
          

          
          
    <div class="social-share">
      <span>Share:</span>
    </div>



        </div>
      
      
      
    </footer>
  </div>
</article>


  <article id="post-Python-100-Days-master/Day41-55/54.单元测试" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-inner">
    
    
    <div class="article-meta">
      
      <span class="article-date">
  <i class="fa fa-date"></i>
  <time class="dt-published" datetime="2021-10-17T13:24:55.327Z" itemprop="datePublished">2021年10月17日</time>
</span>
      
      
      
<a href="/2021/10/17/Python-100-Days-master/Day41-55/54.%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95/#comments" class="article-comment-link">
  
  <i class="fa fa-commt"></i>
  Guestbook
</a>


    </div>
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="单元测试"><a href="#单元测试" class="headerlink" title="单元测试"></a>单元测试</h2><p>Python标准库中提供了名为<code>unittest</code> 的模块来支持我们对代码进行单元测试。所谓单元测试是指针对程序中最小的功能单元（在Python中指函数或类中的方法）进行的测试。</p>

        
        
          <blockquote id="copyright">
              <p>Original link: <a href="http://example.com/page/6/index.html">http://example.com/page/6/index.html</a></p>
              <p>Copyright Notice: 转载请注明出处.</p>
          </blockquote>
        
      
    </div>
    <footer class="article-footer">
      
        <div class="article-tag-wrap">
          

          
          
    <div class="social-share">
      <span>Share:</span>
    </div>



        </div>
      
      
      
    </footer>
  </div>
</article>



    <nav id="page-nav">

<a class="extend prev" rel="prev" href="/page/5/">Previous</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><a class="page-number" href="/page/5/">5</a><span class="page-number current">6</span><a class="page-number" href="/page/7/">7</a><a class="page-number" href="/page/8/">8</a><span class="space">&hellip;</span><a class="page-number" href="/page/18/">18</a><a class="extend next" rel="next" href="/page/7/">Next</a>
</nav>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title"><i class="fa fa-posts"></i> Recent</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2021/10/17/Python-100-Days-master/%E7%95%AA%E5%A4%96%E7%AF%87/%E9%82%A3%E4%BA%9B%E5%B9%B4%E6%88%91%E4%BB%AC%E8%B8%A9%E8%BF%87%E7%9A%84%E9%82%A3%E4%BA%9B%E5%9D%91/">(no title)</a>
          </li>
        
          <li>
            <a href="/2021/10/17/Python-100-Days-master/%E7%95%AA%E5%A4%96%E7%AF%87/%E8%8B%B1%E8%AF%AD%E9%9D%A2%E8%AF%95/">(no title)</a>
          </li>
        
          <li>
            <a href="/2021/10/17/Python-100-Days-master/%E7%95%AA%E5%A4%96%E7%AF%87/%E7%9F%A5%E4%B9%8E%E9%97%AE%E9%A2%98%E5%9B%9E%E7%AD%94/">(no title)</a>
          </li>
        
          <li>
            <a href="/2021/10/17/Python-100-Days-master/%E7%95%AA%E5%A4%96%E7%AF%87/%E7%94%A8%E5%87%BD%E6%95%B0%E8%BF%98%E6%98%AF%E7%94%A8%E5%A4%8D%E6%9D%82%E7%9A%84%E8%A1%A8%E8%BE%BE%E5%BC%8F/">(no title)</a>
          </li>
        
          <li>
            <a href="/2021/10/17/Python-100-Days-master/%E7%95%AA%E5%A4%96%E7%AF%87/%E7%8E%A9%E8%BD%ACPyCharm/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title"><i class="fa fa-tag"></i> Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/PEP8/" style="font-size: 10px;">PEP8</a> <a href="/tags/PEP899/" style="font-size: 10px;">PEP899</a> <a href="/tags/Python/" style="font-size: 10px;">Python</a> <a href="/tags/Python99/" style="font-size: 10px;">Python99</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title"><i class="fa fa-classify"></i> Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Python%E5%9F%BA%E7%A1%80/">Python基础</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python%E5%9F%BA%E7%A1%8000/">Python基础00</a><span class="category-list-count">1</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title"><i class="fa fa-archive"></i> Archive</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/">2021年10月</a><span class="archive-list-count">173</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/">2019年08月</a><span class="archive-list-count">2</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title"><i class="fa fa-tag"></i> Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/PEP8/" rel="tag">PEP8</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PEP899/" rel="tag">PEP899</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/" rel="tag">Python</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python99/" rel="tag">Python99</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title"><i class="fa fa-link"></i> Blogroll</h3>
    <div class="widget">
      <ul>
      
        <li>
          <a target="_blank" rel="noopener" href="http://www.example1.com/">site-name1</a>
        </li>
      
        <li>
          <a target="_blank" rel="noopener" href="http://www.example2.com/">site-name2</a>
        </li>
      
        <li>
          <a target="_blank" rel="noopener" href="http://www.example3.com/">site-name3</a>
        </li>
      
      </ul>
    </div>
  </div>


  
</aside>
        
      </div>
      <a id="totop" href="#top"></a>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      <p>
        <a href="/sitemap.xml">Site Map</a>
        <span> | </span><a href="/atom.xml">Subscribe to this site</a>
        <span> | </span><a href="/about/">Contact the blogger</a>
      </p>
      
        <p>
          <i class="fa fa-visitors"></i>
          <i id="busuanzi_container_site_uv"><i id="busuanzi_value_site_uv"></i></i>
          ，
          <i class="fa fa-views"></i>
          <i id="busuanzi_container_site_pv"><i id="busuanzi_value_site_pv"></i></i>
        </p>
      
      <p>
        <span>Copyright &copy; 2021 John Doe.</span>
        <span>Theme by <a href="https://github.com/chaooo/hexo-theme-BlueLake/" target="_blank">BlueLake.</a></span>
        
            <span>Count by <a href="http://busuanzi.ibruce.info/" target="_blank">busuanzi.</a></span>
        
        <span>Powered by <a href="https://hexo.io/" target="_blank">Hexo.</a></span>
      </p>
    </div>
  </div>
</footer>

    </div>
    
<script src="/js/jquery-3.4.1.min.js"></script>


<script src="/js/search.json.js"></script>


  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>






  
<script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>



  
<script src="/localshare/js/social-share.js"></script>

  
<script src="/localshare/js/qrcode.js"></script>




















  </div>
</body>
</html>