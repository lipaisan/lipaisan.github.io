<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
    
<link rel="stylesheet" href="/localshare/css/share.css">

  
  
<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        
          <a class="main-nav-link" href="/."><i class="fa fa-home"></i> Home</a>
        
          <a class="main-nav-link" href="/archives/"><i class="fa fa-archive"></i> Archive</a>
        
          <a class="main-nav-link" href="/about/"><i class="fa fa-user"></i> About</a>
        
          <a class="main-nav-link" href="/atom.xml"><i class="fa fa-rss"></i> RSS</a>
        
      </nav>
    </div>
    <div id="search-form">
      <div id="result-mask" class="hide"></div>
      <label><input id="search-key" type="text" autocomplete="off" placeholder="search"></label>
      <div id="result-wrap" class="hide">
        <div id="search-result"></div>
      </div>
      <div class="hide">
        <template id="search-tpl">
          <div class="item">
            <a href="/{path}" title="{title}">
              <div class="title">{title}</div>
              <div class="time">{date}</div>
              <div class="tags">{tags}</div>
            </a>
          </div>
        </template>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-Python-100-Days-master/Day61-65/64.并发下载" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-inner">
    
    
    <div class="article-meta">
      
      <span class="article-date">
  <i class="fa fa-date"></i>
  <time class="dt-published" datetime="2021-10-17T13:24:55.578Z" itemprop="datePublished">2021年10月17日</time>
</span>
      
      
        <span class="article-views">
  <i class="fa fa-views"></i>
  <i id="busuanzi_container_page_pv">
      <i id="busuanzi_value_page_pv"></i>
  </i>
</span>

      
      
<a href="/2021/10/17/Python-100-Days-master/Day61-65/64.%E5%B9%B6%E5%8F%91%E4%B8%8B%E8%BD%BD/#comments" class="article-comment-link">
  
    
    
    
    
    
  
  <i class="fa fa-commt"></i>
  Guestbook
</a>


    </div>
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="并发下载"><a href="#并发下载" class="headerlink" title="并发下载"></a>并发下载</h2><h3 id="多线程和多进程补充知识点"><a href="#多线程和多进程补充知识点" class="headerlink" title="多线程和多进程补充知识点"></a>多线程和多进程补充知识点</h3><h4 id="threading-local类"><a href="#threading-local类" class="headerlink" title="threading.local类"></a>threading.local类</h4><p>使用线程时最不愿意遇到的情况就是多个线程竞争资源，在这种情况下为了保证资源状态的正确性，我们可能需要对资源进行加锁保护的处理，这一方面会导致程序失去并发性，另外如果多个线程竞争多个资源时，还有可能因为加锁方式的不当导致<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E6%AD%BB%E9%94%81">死锁</a>。要解决多个线程竞争资源的问题，其中一个方案就是让每个线程都持有资源的副本（拷贝），这样每个线程可以操作自己所持有的资源，从而规避对资源的竞争。</p>
<p>要实现将资源和持有资源的线程进行绑定的操作，最简单的做法就是使用<code>threading</code>模块的<code>local</code>类，在网络爬虫开发中，就可以使用<code>local</code>类为每个线程绑定一个MySQL数据库连接或Redis客户端对象，这样通过线程可以直接获得这些资源，既解决了资源竞争的问题，又避免了在函数和方法调用时传递这些资源。具体的请参考本章多线程爬取“手机搜狐网”（Redis版）的实例代码。</p>
<h4 id="concurrent-futures模块"><a href="#concurrent-futures模块" class="headerlink" title="concurrent.futures模块"></a>concurrent.futures模块</h4><p>Python3.2带来了<code>concurrent.futures</code> 模块，这个模块包含了线程池和进程池、管理并行编程任务、处理非确定性的执行流程、进程/线程同步等功能。关于这部分的内容推荐大家阅读<a target="_blank" rel="noopener" href="http://python-parallel-programmning-cookbook.readthedocs.io/zh_CN/latest/index.html">《Python并行编程》</a>。</p>
<h4 id="分布式进程"><a href="#分布式进程" class="headerlink" title="分布式进程"></a>分布式进程</h4><p>使用多进程的时候，可以将进程部署在多个主机节点上，Python的<code>multiprocessing</code>模块不但支持多进程，其中<code>managers</code>子模块还支持把多进程部署到多个节点上。当然，要部署分布式进程，首先需要一个服务进程作为调度者，进程之间通过网络进行通信来实现对进程的控制和调度，由于<code>managers</code>模块已经对这些做出了很好的封装，因此在无需了解网络通信细节的前提下，就可以编写分布式多进程应用。具体的请参照本章分布式多进程爬取“手机搜狐网”的实例代码。</p>
<h3 id="协程和异步I-O"><a href="#协程和异步I-O" class="headerlink" title="协程和异步I/O"></a>协程和异步I/O</h3><h4 id="协程的概念"><a href="#协程的概念" class="headerlink" title="协程的概念"></a>协程的概念</h4><p>协程（coroutine）通常又称之为微线程或纤程，它是相互协作的一组子程序（函数）。所谓相互协作指的是在执行函数A时，可以随时中断去执行函数B，然后又中断继续执行函数A。注意，这一过程并不是函数调用（因为没有调用语句），整个过程看似像多线程，然而协程只有一个线程执行。协程通过<code>yield</code>关键字和 <code>send()</code>操作来转移执行权，协程之间不是调用者与被调用者的关系。</p>
<p>协程的优势在于以下两点：</p>
<ol>
<li>执行效率极高，因为子程序（函数）切换不是线程切换，由程序自身控制，没有切换线程的开销。</li>
<li>不需要多线程的锁机制，因为只有一个线程，也不存在竞争资源的问题，当然也就不需要对资源加锁保护，因此执行效率高很多。</li>
</ol>
<blockquote>
<p><strong>说明</strong>：协程适合处理的是I/O密集型任务，处理CPU密集型任务并不是它擅长的，如果要提升CPU的利用率可以考虑“多进程+多线程”或者“多进程+协程”的工作模式。</p>
</blockquote>
<h4 id="历史回顾"><a href="#历史回顾" class="headerlink" title="历史回顾"></a>历史回顾</h4><ol>
<li>Python 2.2：第一次提出了生成器（最初称之为迭代器）的概念（PEP 255）。</li>
<li>Python 2.5：引入了将对象发送回暂停了的生成器这一特性即生成器的<code>send()</code>方法（PEP 342）。</li>
<li>Python 3.3：添加了<code>yield from</code>特性，允许从迭代器中返回任何值（注意生成器本身也是迭代器），这样我们就可以串联生成器并且重构出更好的生成器。</li>
<li>Python 3.4：引入<code>asyncio.coroutine</code>装饰器用来标记作为协程的函数，协程函数和<code>asyncio</code>及其事件循环一起使用，来实现异步I/O操作。</li>
<li>Python 3.5：引入了<code>async</code>和<code>await</code>，可以使用<code>async def</code>来定义一个协程函数，这个函数中不能包含任何形式的<code>yield</code>语句，但是可以使用<code>return</code>或<code>await</code>从协程中返回值。</li>
</ol>
<p>协程实现了协作式并发，通过提高CPU的利用率来达到改善性能的目的。著名的三方库<a target="_blank" rel="noopener" href="https://github.com/aio-libs/aiohttp"><code>aiohttp</code></a>就是通过协程的方式实现了HTTP客户端和HTTP服务器的功能，较之<code>requests</code>有更好的获取数据的性能，有兴趣可以阅读它的<a target="_blank" rel="noopener" href="https://aiohttp.readthedocs.io/en/stable/">官方文档</a>。</p>
<pre><code class="Python">import asyncio
import aiohttp


async def download(url):
    print(&#39;Fetch:&#39;, url)
    async with aiohttp.ClientSession() as session:
        async with session.get(url, ssl=False) as resp:
            print(url, &#39;---&gt;&#39;, resp.status)
            print(url, &#39;---&gt;&#39;, resp.headers)
            print(&#39;\n\n&#39;, await resp.text())


def main():
    loop = asyncio.get_event_loop()
    urls = [
        &#39;https://www.baidu.com&#39;,
        &#39;http://www.sohu.com/&#39;,
        &#39;http://www.sina.com.cn/&#39;,
        &#39;https://www.taobao.com/&#39;,
        &#39;http://jd.com/&#39;
    ]
    tasks = [download(url) for url in urls]
    loop.run_until_complete(asyncio.wait(tasks))
    loop.close()


if __name__ == &#39;__main__&#39;:
    main()
</code></pre>
<h3 id="实例-多线程爬取“手机搜狐网”所有页面"><a href="#实例-多线程爬取“手机搜狐网”所有页面" class="headerlink" title="实例 - 多线程爬取“手机搜狐网”所有页面"></a>实例 - 多线程爬取“手机搜狐网”所有页面</h3><p>下面我们把之间讲的所有知识结合起来，用面向对象的方式实现一个爬取“手机搜狐网”的多线程爬虫。</p>
<pre><code class="Python">import pickle
import zlib
from enum import Enum, unique
from hashlib import sha1
from random import random
from threading import Thread, current_thread, local
from time import sleep
from urllib.parse import urlparse

import pymongo
import redis
import requests
from bs4 import BeautifulSoup
from bson import Binary


@unique
class SpiderStatus(Enum):
    IDLE = 0
    WORKING = 1


def decode_page(page_bytes, charsets=(&#39;utf-8&#39;,)):
    page_html = None
    for charset in charsets:
        try:
            page_html = page_bytes.decode(charset)
            break
        except UnicodeDecodeError:
            pass
    return page_html


class Retry(object):

    def __init__(self, *, retry_times=3,
                 wait_secs=5, errors=(Exception, )):
        self.retry_times = retry_times
        self.wait_secs = wait_secs
        self.errors = errors

    def __call__(self, fn):

        def wrapper(*args, **kwargs):
            for _ in range(self.retry_times):
                try:
                    return fn(*args, **kwargs)
                except self.errors as e:
                    print(e)
                    sleep((random() + 1) * self.wait_secs)
            return None

        return wrapper


class Spider(object):

    def __init__(self):
        self.status = SpiderStatus.IDLE

    @Retry()
    def fetch(self, current_url, *, charsets=(&#39;utf-8&#39;, ),
              user_agent=None, proxies=None):
        thread_name = current_thread().name
        print(f&#39;[&#123;thread_name&#125;]: &#123;current_url&#125;&#39;)
        headers = &#123;&#39;user-agent&#39;: user_agent&#125; if user_agent else &#123;&#125;
        resp = requests.get(current_url,
                            headers=headers, proxies=proxies)
        return decode_page(resp.content, charsets) \
            if resp.status_code == 200 else None

    def parse(self, html_page, *, domain=&#39;m.sohu.com&#39;):
        soup = BeautifulSoup(html_page, &#39;lxml&#39;)
        for a_tag in soup.body.select(&#39;a[href]&#39;):
            parser = urlparse(a_tag.attrs[&#39;href&#39;])
            scheme = parser.scheme or &#39;http&#39;
            netloc = parser.netloc or domain
            if scheme != &#39;javascript&#39; and netloc == domain:
                path = parser.path
                query = &#39;?&#39; + parser.query if parser.query else &#39;&#39;
                full_url = f&#39;&#123;scheme&#125;://&#123;netloc&#125;&#123;path&#125;&#123;query&#125;&#39;
                redis_client = thread_local.redis_client
                if not redis_client.sismember(&#39;visited_urls&#39;, full_url):
                    redis_client.rpush(&#39;m_sohu_task&#39;, full_url)

    def extract(self, html_page):
        pass

    def store(self, data_dict):
        # redis_client = thread_local.redis_client
        # mongo_db = thread_local.mongo_db
        pass


class SpiderThread(Thread):

    def __init__(self, name, spider):
        super().__init__(name=name, daemon=True)
        self.spider = spider

    def run(self):
        redis_client = redis.Redis(host=&#39;1.2.3.4&#39;, port=6379, password=&#39;1qaz2wsx&#39;)
        mongo_client = pymongo.MongoClient(host=&#39;1.2.3.4&#39;, port=27017)
        thread_local.redis_client = redis_client
        thread_local.mongo_db = mongo_client.msohu 
        while True:
            current_url = redis_client.lpop(&#39;m_sohu_task&#39;)
            while not current_url:
                current_url = redis_client.lpop(&#39;m_sohu_task&#39;)
            self.spider.status = SpiderStatus.WORKING
            current_url = current_url.decode(&#39;utf-8&#39;)
            if not redis_client.sismember(&#39;visited_urls&#39;, current_url):
                redis_client.sadd(&#39;visited_urls&#39;, current_url)
                html_page = self.spider.fetch(current_url)
                if html_page not in [None, &#39;&#39;]:
                    hasher = hasher_proto.copy()
                    hasher.update(current_url.encode(&#39;utf-8&#39;))
                    doc_id = hasher.hexdigest()
                    sohu_data_coll = mongo_client.msohu.webpages
                    if not sohu_data_coll.find_one(&#123;&#39;_id&#39;: doc_id&#125;):
                        sohu_data_coll.insert_one(&#123;
                            &#39;_id&#39;: doc_id,
                            &#39;url&#39;: current_url,
                            &#39;page&#39;: Binary(zlib.compress(pickle.dumps(html_page)))
                        &#125;)
                    self.spider.parse(html_page)
            self.spider.status = SpiderStatus.IDLE


def is_any_alive(spider_threads):
    return any([spider_thread.spider.status == SpiderStatus.WORKING
                for spider_thread in spider_threads])


thread_local = local()
hasher_proto = sha1()


def main():
    redis_client = redis.Redis(host=&#39;1.2.3.4&#39;, port=6379, password=&#39;1qaz2wsx&#39;)
    if not redis_client.exists(&#39;m_sohu_task&#39;):
        redis_client.rpush(&#39;m_sohu_task&#39;, &#39;http://m.sohu.com/&#39;)

    spider_threads = [SpiderThread(&#39;thread-%d&#39; % i, Spider())
                      for i in range(10)]
    for spider_thread in spider_threads:
        spider_thread.start()

    while redis_client.exists(&#39;m_sohu_task&#39;) or is_any_alive(spider_threads):
        sleep(5)

    print(&#39;Over!&#39;)


if __name__ == &#39;__main__&#39;:
    main()
</code></pre>

        
            <div id="toc-article">
                
  <div class="widget-wrap" id="toc-wrap">
    <h3 class="widget-title"><i class="fa fa-toc"></i> Contents</h3>
    <div class="widget">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B9%B6%E5%8F%91%E4%B8%8B%E8%BD%BD"><span class="toc-text">并发下载</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%92%8C%E5%A4%9A%E8%BF%9B%E7%A8%8B%E8%A1%A5%E5%85%85%E7%9F%A5%E8%AF%86%E7%82%B9"><span class="toc-text">多线程和多进程补充知识点</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#threading-local%E7%B1%BB"><span class="toc-text">threading.local类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#concurrent-futures%E6%A8%A1%E5%9D%97"><span class="toc-text">concurrent.futures模块</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E8%BF%9B%E7%A8%8B"><span class="toc-text">分布式进程</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%8F%E7%A8%8B%E5%92%8C%E5%BC%82%E6%AD%A5I-O"><span class="toc-text">协程和异步I&#x2F;O</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8D%8F%E7%A8%8B%E7%9A%84%E6%A6%82%E5%BF%B5"><span class="toc-text">协程的概念</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8E%86%E5%8F%B2%E5%9B%9E%E9%A1%BE"><span class="toc-text">历史回顾</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E4%BE%8B-%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%88%AC%E5%8F%96%E2%80%9C%E6%89%8B%E6%9C%BA%E6%90%9C%E7%8B%90%E7%BD%91%E2%80%9D%E6%89%80%E6%9C%89%E9%A1%B5%E9%9D%A2"><span class="toc-text">实例 - 多线程爬取“手机搜狐网”所有页面</span></a></li></ol></li></ol>
    </div>
  </div>


            </div>
        
        
          <blockquote id="copyright">
              <p>Original link: <a href="http://example.com/2021/10/17/Python-100-Days-master/Day61-65/64.并发下载/">http://example.com/2021/10/17/Python-100-Days-master/Day61-65/64.并发下载/</a></p>
              <p>Copyright Notice: 转载请注明出处.</p>
          </blockquote>
        
      
    </div>
    <footer class="article-footer">
      
        <div class="article-tag-wrap">
          

          
          
    <div class="social-share">
      <span>Share:</span>
    </div>



        </div>
      
      
        
<nav id="article-nav">
  
    <a href="/2021/10/17/Python-100-Days-master/Day61-65/65.%E8%A7%A3%E6%9E%90%E5%8A%A8%E6%80%81%E5%86%85%E5%AE%B9/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">newer</strong>
      <div class="article-nav-title">
        
          (no title)
        
      </div>
    </a>
  
  
    <a href="/2021/10/17/Python-100-Days-master/Day61-65/63.%E5%AD%98%E5%82%A8%E6%95%B0%E6%8D%AE/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">older</strong>
      <div class="article-nav-title"></div>
    </a>
  
</nav>

      
      
        







      
    </footer>
  </div>
</article>
</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title"><i class="fa fa-posts"></i> Recent</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2021/10/17/Python-100-Days-master/%E7%95%AA%E5%A4%96%E7%AF%87/%E9%82%A3%E4%BA%9B%E5%B9%B4%E6%88%91%E4%BB%AC%E8%B8%A9%E8%BF%87%E7%9A%84%E9%82%A3%E4%BA%9B%E5%9D%91/">(no title)</a>
          </li>
        
          <li>
            <a href="/2021/10/17/Python-100-Days-master/%E7%95%AA%E5%A4%96%E7%AF%87/%E8%8B%B1%E8%AF%AD%E9%9D%A2%E8%AF%95/">(no title)</a>
          </li>
        
          <li>
            <a href="/2021/10/17/Python-100-Days-master/%E7%95%AA%E5%A4%96%E7%AF%87/%E7%9F%A5%E4%B9%8E%E9%97%AE%E9%A2%98%E5%9B%9E%E7%AD%94/">(no title)</a>
          </li>
        
          <li>
            <a href="/2021/10/17/Python-100-Days-master/%E7%95%AA%E5%A4%96%E7%AF%87/%E7%94%A8%E5%87%BD%E6%95%B0%E8%BF%98%E6%98%AF%E7%94%A8%E5%A4%8D%E6%9D%82%E7%9A%84%E8%A1%A8%E8%BE%BE%E5%BC%8F/">(no title)</a>
          </li>
        
          <li>
            <a href="/2021/10/17/Python-100-Days-master/%E7%95%AA%E5%A4%96%E7%AF%87/%E7%8E%A9%E8%BD%ACPyCharm/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title"><i class="fa fa-tag"></i> Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/PEP8/" style="font-size: 10px;">PEP8</a> <a href="/tags/PEP899/" style="font-size: 10px;">PEP899</a> <a href="/tags/Python/" style="font-size: 10px;">Python</a> <a href="/tags/Python99/" style="font-size: 10px;">Python99</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title"><i class="fa fa-classify"></i> Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Python%E5%9F%BA%E7%A1%80/">Python基础</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Python%E5%9F%BA%E7%A1%8000/">Python基础00</a><span class="category-list-count">1</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title"><i class="fa fa-archive"></i> Archive</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/">2021年10月</a><span class="archive-list-count">173</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/">2019年08月</a><span class="archive-list-count">2</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title"><i class="fa fa-tag"></i> Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/PEP8/" rel="tag">PEP8</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PEP899/" rel="tag">PEP899</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/" rel="tag">Python</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python99/" rel="tag">Python99</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title"><i class="fa fa-link"></i> Blogroll</h3>
    <div class="widget">
      <ul>
      
        <li>
          <a target="_blank" rel="noopener" href="http://www.example1.com/">site-name1</a>
        </li>
      
        <li>
          <a target="_blank" rel="noopener" href="http://www.example2.com/">site-name2</a>
        </li>
      
        <li>
          <a target="_blank" rel="noopener" href="http://www.example3.com/">site-name3</a>
        </li>
      
      </ul>
    </div>
  </div>


  
</aside>
        
      </div>
      <a id="totop" href="#top"></a>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      <p>
        <a href="/sitemap.xml">Site Map</a>
        <span> | </span><a href="/atom.xml">Subscribe to this site</a>
        <span> | </span><a href="/about/">Contact the blogger</a>
      </p>
      
        <p>
          <i class="fa fa-visitors"></i>
          <i id="busuanzi_container_site_uv"><i id="busuanzi_value_site_uv"></i></i>
          ，
          <i class="fa fa-views"></i>
          <i id="busuanzi_container_site_pv"><i id="busuanzi_value_site_pv"></i></i>
        </p>
      
      <p>
        <span>Copyright &copy; 2021 John Doe.</span>
        <span>Theme by <a href="https://github.com/chaooo/hexo-theme-BlueLake/" target="_blank">BlueLake.</a></span>
        
            <span>Count by <a href="http://busuanzi.ibruce.info/" target="_blank">busuanzi.</a></span>
        
        <span>Powered by <a href="https://hexo.io/" target="_blank">Hexo.</a></span>
      </p>
    </div>
  </div>
</footer>

    </div>
    
<script src="/js/jquery-3.4.1.min.js"></script>


<script src="/js/search.json.js"></script>


  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>






  
<script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>



  
<script src="/localshare/js/social-share.js"></script>

  
<script src="/localshare/js/qrcode.js"></script>




















  </div>
</body>
</html>